---
slug: a-kind-fast-llama-13b-on-rtx-2060
title: "A Kind Llama 13b chat model on my RTX 2060 laptop"
authors:
- name: Sam Stoelinga
  title: Engineer
  url: https://github.com/samos123
  image_url: https://avatars.githubusercontent.com/u/10274189?v=4
image: /img/kubectl-notebook-cmd.small.png
tags: [llama, kind]
---

<img src="/img/kubectl-notebook-cmd.png" alt="kubectl notebook" width="100%" />

import GitHubButton from 'react-github-btn'

I was pleasantly surprised by how well the Llama 13b chat model was able to
run on my laptop that has an RTX 2060. The RTX 2060 has just 6GB of GPU memory.
What was even cooler is that it all ran on a local K8s cluster on my laptop
by using Kind.

<GitHubButton href="https://github.com/substratusai/substratus" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star substratusai/substratus on GitHub">Star</GitHubButton>

Don't forget to star and follow the repo!

