<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Substratus Blog</title>
        <link>https://www.substratus.ai/blog</link>
        <description>Substratus Blog</description>
        <lastBuildDate>Thu, 07 Sep 2023 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Tutorial: K8s Kind with GPUs]]></title>
            <link>https://www.substratus.ai/blog/kind-with-gpus</link>
            <guid>https://www.substratus.ai/blog/kind-with-gpus</guid>
            <pubDate>Thu, 07 Sep 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Don't you just love it when you submit a PR and it turns out that no code is]]></description>
            <content:encoded><![CDATA[<div class="video-container"><iframe class="video" src="https://www.youtube-nocookie.com/embed/O1683vzaJVE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"></iframe></div><p>Don't you just love it when you submit a PR and it turns out that no code is
needed? That's exactly what happened when I tried add GPU support to Kind.</p><p>In this blog post you will learn how to configure Kind such
that it can use the GPUs on your device. Credit to
<a href="https://github.com/kubernetes-sigs/kind/pull/3257#issuecomment-1607287275" target="_blank" rel="noopener noreferrer">@klueska</a>
for the solution.</p><p>Install the NVIDIA container toolkit by following the <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/user-guide.html#adding-the-nvidia-runtime" target="_blank" rel="noopener noreferrer">official install docs</a>.</p><p>Configure NVIDIA to be the default runtime for docker:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token function" style="color:#d73a49">sudo</span><span class="token plain"> nvidia-ctk runtime configure --runtime</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">docker --set-as-default</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token function" style="color:#d73a49">sudo</span><span class="token plain"> systemctl restart </span><span class="token function" style="color:#d73a49">docker</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Set <code>accept-nvidia-visible-devices-as-volume-mounts = true</code> in <code>/etc/nvidia-container-runtime/config.toml</code>:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token function" style="color:#d73a49">sudo</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">sed</span><span class="token plain"> -i </span><span class="token string" style="color:#e3116c">'/accept-nvidia-visible-devices-as-volume-mounts/c\accept-nvidia-visible-devices-as-volume-mounts = true'</span><span class="token plain"> /etc/nvidia-container-runtime/config.toml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Create a Kind Cluster:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kind create cluster --name substratus --config - </span><span class="token operator" style="color:#393A34">&lt;&lt;</span><span class="token string" style="color:#e3116c">EOF</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">apiVersion: kind.x-k8s.io/v1alpha4</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">kind: Cluster</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">nodes:</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">- role: control-plane</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">  image: kindest/node:v1.27.3@sha256:3966ac761ae0136263ffdb6cfd4db23ef8a83cba8a463690e98317add2c9ba72</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">  # required for GPU workaround</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">  extraMounts:</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    - hostPath: /dev/null</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">      containerPath: /var/run/nvidia-container-devices/all</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">EOF</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Workaround for issue with missing required file <code>/sbin/ldconfig.real</code>:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># https://github.com/NVIDIA/nvidia-docker/issues/614#issuecomment-423991632</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token function" style="color:#d73a49">docker</span><span class="token plain"> </span><span class="token builtin class-name">exec</span><span class="token plain"> -ti substratus-control-plane </span><span class="token function" style="color:#d73a49">ln</span><span class="token plain"> -s /sbin/ldconfig /sbin/ldconfig.real</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Install the K8s NVIDIA GPU operator so K8s is aware of your NVIDIA device:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">helm repo </span><span class="token function" style="color:#d73a49">add</span><span class="token plain"> nvidia https://helm.ngc.nvidia.com/nvidia </span><span class="token operator" style="color:#393A34">||</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">true</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">helm repo update</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">helm </span><span class="token function" style="color:#d73a49">install</span><span class="token plain"> --wait --generate-name </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     -n gpu-operator --create-namespace </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     nvidia/gpu-operator --set driver.enabled</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">false</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>You should now have a working Kind cluster that can access your GPU.
Verify it by running a simple pod:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl apply -f - </span><span class="token operator" style="color:#393A34">&lt;&lt;</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">EOF</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">apiVersion: v1</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">kind: Pod</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">metadata:</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">  name: cuda-vectoradd</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">spec:</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">  restartPolicy: OnFailure</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">  containers:</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">  - name: cuda-vectoradd</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    image: "nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda11.7.1-ubuntu20.04"</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    resources:</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">      limits:</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">        nvidia.com/gpu: 1</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">EOF</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>]]></content:encoded>
            <category>kind</category>
            <category>gpu</category>
        </item>
        <item>
            <title><![CDATA[Converting HuggingFace Models to GGUF/GGML]]></title>
            <link>https://www.substratus.ai/blog/converting-hf-model-gguf-model</link>
            <guid>https://www.substratus.ai/blog/converting-hf-model-gguf-model</guid>
            <pubDate>Thu, 31 Aug 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Llama.cpp is a great way to run LLMs efficiently on CPUs and GPUs. The downside]]></description>
            <content:encoded><![CDATA[<p>Llama.cpp is a great way to run LLMs efficiently on CPUs and GPUs. The downside
however is that you need to convert models to a format that's supported by Llama.cpp,
which is now the GGUF file format.  In this blog post you will learn how to convert
a HuggingFace model (Vicuna 13b v1.5) to GGUF model.</p><p>At the time of writing, Llama.cpp supports
the following models:</p><ul><li>LLaMA 🦙</li><li>LLaMA 2 🦙🦙</li><li>Falcon</li><li>Alpaca</li><li>GPT4All</li><li>Chinese LLaMA / Alpaca and Chinese LLaMA-2 / Alpaca-2</li><li>Vigogne (French)</li><li>Vicuna</li><li>Koala</li><li>OpenBuddy 🐶 (Multilingual)</li><li>Pygmalion 7B / Metharme 7B</li><li>WizardLM</li><li>Baichuan-7B and its derivations (such as baichuan-7b-sft)</li><li>Aquila-7B / AquilaChat-7B</li></ul><p>At a high-level you will be going through the following steps:</p><ul><li>Downloading a HuggingFace model</li><li>Running llama.cpp <code>convert.py</code> on the HuggingFace model</li><li>(Optionally) Uploading the model back to HuggingFace</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="downloading-a-huggingface-model">Downloading a HuggingFace model<a href="#downloading-a-huggingface-model" class="hash-link" aria-label="Direct link to Downloading a HuggingFace model" title="Direct link to Downloading a HuggingFace model">​</a></h3><p>There are various ways to download models, but in my experience the <code>huggingface_hub</code>
library has been the most reliable. The <code>git clone</code> method occasionally results in
OOM errors for large models.</p><p>Install the <code>huggingface_hub</code> library:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip </span><span class="token function" style="color:#d73a49">install</span><span class="token plain"> huggingface_hub</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Create a Python script named <code>download.py</code> with the following content:</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> huggingface_hub </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> snapshot_download</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model_id</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">"lmsys/vicuna-13b-v1.5"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">snapshot_download</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">repo_id</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">model_id</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> local_dir</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">"vicuna-hf"</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                  local_dir_use_symlinks</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">False</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> revision</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">"main"</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Run the Python script:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">python download.py</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>You should now have the model downloaded to a directory called
<code>vicuna-hf</code>. Verify by running:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token function" style="color:#d73a49">ls</span><span class="token plain"> -lash vicuna-hf</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="converting-the-model">Converting the model<a href="#converting-the-model" class="hash-link" aria-label="Direct link to Converting the model" title="Direct link to Converting the model">​</a></h3><p>Now it's time to convert the downloaded HuggingFace model to a GGUF model.
Llama.cpp comes with a converter script to do this.</p><p>Get the script by cloning the llama.cpp repo:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token function" style="color:#d73a49">git</span><span class="token plain"> clone https://github.com/ggerganov/llama.cpp.git</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Install the required python libraries:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip </span><span class="token function" style="color:#d73a49">install</span><span class="token plain"> -r llama.cpp/requirements.txt</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Verify the script is there and understand the various options:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">python llama.cpp/convert.py -h</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Convert the HF model to GGUF model:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">python llama.cpp/convert.py vicuna-hf </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --outfile vicuna-13b-v1.5.gguf </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --outtype q8_0</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>In this case we're also quantizing the model to 8 bit by setting
<code>--outtype q8_0</code>. Quantizing helps improve inference speed, but it can
negatively impact quality.
You can use <code>--outtype f16</code> (16 bit) or <code>--outtype f32</code> (32 bit) to preserve original
quality.</p><p>Verify the GGUF model was created:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token function" style="color:#d73a49">ls</span><span class="token plain"> -lash vicuna-13b-v1.5.gguf</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="pushing-the-gguf-model-to-huggingface">Pushing the GGUF model to HuggingFace<a href="#pushing-the-gguf-model-to-huggingface" class="hash-link" aria-label="Direct link to Pushing the GGUF model to HuggingFace" title="Direct link to Pushing the GGUF model to HuggingFace">​</a></h3><p>You can optionally push back the GGUF model to HuggingFace.</p><p>Create a Python script with the filename <code>upload.py</code> that
has the following content:</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> huggingface_hub </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> HfApi</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">api </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> HfApi</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model_id </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">"substratusai/vicuna-13b-v1.5-gguf"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">api</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">create_repo</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">model_id</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> exist_ok</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> repo_type</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">"model"</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">api</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">upload_file</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    path_or_fileobj</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">"vicuna-13b-v1.5.gguf"</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    path_in_repo</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">"vicuna-13b-v1.5.gguf"</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    repo_id</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">model_id</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Get a HuggingFace Token that has write permission from here:
<a href="https://huggingface.co/settings/tokens" target="_blank" rel="noopener noreferrer">https://huggingface.co/settings/tokens</a></p><p>Set your HuggingFace token:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">export</span><span class="token plain"> </span><span class="token assign-left variable" style="color:#36acaa">HUGGING_FACE_HUB_TOKEN</span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">paste-your-own-token</span><span class="token operator" style="color:#393A34">&gt;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Run the <code>upload.py</code> script:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">python upload.py</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Interested in learning how to automate flows like this? Checkout our
open source project:</p><span><a href="https://github.com/substratusai/substratus" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star substratusai/substratus on GitHub">Star</a></span>]]></content:encoded>
            <category>llama.cpp</category>
            <category>gguf</category>
        </item>
        <item>
            <title><![CDATA[A Kind Local Llama on K8s]]></title>
            <link>https://www.substratus.ai/blog/kind-local-llama-on-rtx-2060</link>
            <guid>https://www.substratus.ai/blog/kind-local-llama-on-rtx-2060</guid>
            <pubDate>Fri, 25 Aug 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[A Llama 13B parameter model running on a laptop with a mere RTX 2060?!]]></description>
            <content:encoded><![CDATA[<img loading="lazy" src="/img/kind-llama-on-laptop-gpu.png" alt="kubectl notebook" width="100%" class="img_ev3q"><p>A Llama 13B parameter model running on a laptop with a mere RTX 2060?!
Yes, it all ran surprisingly well at around 7 tokens / sec.
Follow along and learn how to do this on your environment.</p><p>My laptop setup looks like this:</p><ul><li>Kind for deploying a single node K8s cluster</li><li>AMD Ryzen 7 (8 threads), 16 GB system memory, RTX 2060 (6GB GPU memory)</li><li>Llama.cpp/GGML for fast serving and loading larger models on consumer hardware</li></ul><p>You might be wondering: How can a model with 13 billion parameters fit into a 6GB GPU? You'd expect it to need about 13GB, especially if it's running in 4-bit mode, right?
Yes it should because 13 billion * 4 bytes / (32 bits / 4 bits) = 13 GB.
But thanks to <a href="https://github.com/ggerganov/llama.cpp" target="_blank" rel="noopener noreferrer">Llama.cpp</a>, we can load only parts of the model into the GPU. Plus, Llama.cpp can run efficiently just using the CPU.</p><p>Want to try this out yourself? Follow a long for a fun ride.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="create-kind-k8s-cluster-with-gpu-support">Create Kind K8s cluster with GPU support<a href="#create-kind-k8s-cluster-with-gpu-support" class="hash-link" aria-label="Direct link to Create Kind K8s cluster with GPU support" title="Direct link to Create Kind K8s cluster with GPU support">​</a></h3><p>Install the NVIDIA container toolkit for Docker: <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html" target="_blank" rel="noopener noreferrer">Install Guide</a></p><p>Use the convenience script to create a Kind cluster and configure GPU support:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token function" style="color:#d73a49">bash</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token punctuation" style="color:#393A34">(</span><span class="token function" style="color:#d73a49">curl</span><span class="token plain"> https://raw.githubusercontent.com/substratusai/substratus/main/install/kind/up-gpu.sh</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Or inspect the <a href="https://github.com/substratusai/substratus/blob/main/install/kind/up-gpu.sh" target="_blank" rel="noopener noreferrer">script</a> and run the steps one by one.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="install-substratus">Install Substratus<a href="#install-substratus" class="hash-link" aria-label="Direct link to Install Substratus" title="Direct link to Install Substratus">​</a></h3><p>Install the Substratus K8s operator which will orchestrate model loading and serving:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl apply -f https://raw.githubusercontent.com/substratusai/substratus/main/install/kind/manifests.yaml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="load-the-llama-2-13b-chat-gguf-model">Load the Llama 2 13b chat GGUF model<a href="#load-the-llama-2-13b-chat-gguf-model" class="hash-link" aria-label="Direct link to Load the Llama 2 13b chat GGUF model" title="Direct link to Load the Llama 2 13b chat GGUF model">​</a></h3><p>Create a Model resource to load the <a href="https://huggingface.co/substratusai/Llama-2-13B-chat-GGUF" target="_blank" rel="noopener noreferrer">Llama 2 13b chat GGUF model</a></p><div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token key atrule" style="color:#00a4db">apiVersion</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> substratus.ai/v1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">kind</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> Model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">metadata</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> llama2</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">13b</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">chat</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">gguf</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">spec</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">image</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> substratusai/model</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">loader</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">huggingface</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">params</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> substratusai/Llama</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">2</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">13B</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">chat</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">GGUF</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">files</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">"model.bin"</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl apply -f https://raw.githubusercontent.com/substratusai/substratus/main/examples/llama2-13b-chat-gguf/base-model.yaml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>The model is being downloaded from HuggingFace into your Kind cluster.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="serve-the-model">Serve the model<a href="#serve-the-model" class="hash-link" aria-label="Direct link to Serve the model" title="Direct link to Serve the model">​</a></h3><p>Create a Server resource to serve the model:</p><div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token key atrule" style="color:#00a4db">apiVersion</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> substratus.ai/v1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">kind</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> Server</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">metadata</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> llama2</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">13b</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">chat</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">gguf</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">spec</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">image</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> substratusai/model</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">server</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">llama</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">cpp</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain">latest</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">gpu</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">model</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> llama2</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">13b</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">chat</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">gguf</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">params</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">n_gpu_layers</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">30</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">resources</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">gpu</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token key atrule" style="color:#00a4db">count</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl apply -f https://raw.githubusercontent.com/substratusai/substratus/main/examples/llama2-13b-chat-gguf/server-gpu.yaml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Note in my case 30 out of 42 layers loaded into GPU is the max, but you might be able
to load all 42 layers into the GPU if you have more GPU memory.</p><p>Once the model is ready it will start serving an OpenAI compatible
API endpoint.</p><p>Expose the Server to a local port by using port forwarding:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl port-forward service/llama2-13b-chat-gguf-server </span><span class="token number" style="color:#36acaa">8080</span><span class="token plain">:8080</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Let's throw some prompts at it:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token function" style="color:#d73a49">curl</span><span class="token plain"> http://localhost:8080/v1/completions </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  -H </span><span class="token string" style="color:#e3116c">"Content-Type: application/json"</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  -d </span><span class="token string" style="color:#e3116c">'{ "prompt": "Who was the first president of the United States?", "stop": ["."]}'</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Checkout the full API docs here: <a href="http://localhost:8080/docs" target="_blank" rel="noopener noreferrer">http://localhost:8080/docs</a></p><p>You can play around with other models. For example, if you have a 24 GB GPU card you should
be able to run Llama 2 70B in 4 bit mode by using llama.cpp.</p><p>Support the project by adding a star on GitHub! ❤️</p><span><a href="https://github.com/substratusai/substratus" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star substratusai/substratus on GitHub">Star</a></span>]]></content:encoded>
            <category>llama</category>
            <category>kind</category>
        </item>
        <item>
            <title><![CDATA[Introducing: kubectl notebook]]></title>
            <link>https://www.substratus.ai/blog/introducing-kubectl-notebook</link>
            <guid>https://www.substratus.ai/blog/introducing-kubectl-notebook</guid>
            <pubDate>Tue, 22 Aug 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Substratus has added the kubectl notebook command!]]></description>
            <content:encoded><![CDATA[<img loading="lazy" src="/img/kubectl-notebook-cmd.png" alt="kubectl notebook" width="100%" class="img_ev3q"><p><a href="https://github.com/substratusai/substratus" target="_blank" rel="noopener noreferrer">Substratus</a> has added the <code>kubectl notebook</code> command!</p><blockquote>"Wouldn't it be nice to have a single command that containerized your local directory and served it as a Jupyter Notebook running on a machine with a bunch of GPUs attached?"</blockquote><p>The conversation went something like that while we daydreamed about our preferred workflow. At that point in time we were hopping back-n-forth between Google Colab and our containers while developing a LLM training job.</p><blockquote>"Annnddd it should automatically sync file-changes back to your local directory so that you can commit your changes to git and kick off a long-running ML training job - containerized with the exact same python version and packages!"</blockquote><p>So we built it!</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl notebook -d </span><span class="token builtin class-name">.</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>And now it has become an integral part of our workflow as we build out the <a href="https://github.com/substratusai/substratus" target="_blank" rel="noopener noreferrer">Substratus ML platform</a>.</p><p>Check out the 50 second screenshare:</p><div class="video-container"><iframe class="video" src="https://www.youtube-nocookie.com/embed/0_PWl6vjqdE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"></iframe></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="design-goals">Design Goals<a href="#design-goals" class="hash-link" aria-label="Direct link to Design Goals" title="Direct link to Design Goals">​</a></h2><ol><li>One command should build, launch, and sync the Notebook.</li><li>Users should only need a Kubeconfig - no other credentials.</li><li>Admins should not need to setup networking, TLS, etc.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="implementation">Implementation<a href="#implementation" class="hash-link" aria-label="Direct link to Implementation" title="Direct link to Implementation">​</a></h2><p>We tackled our design goals using the following techniques:</p><ol><li>Implemented as a single Go binary, executed as a <a href="https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/" target="_blank" rel="noopener noreferrer">kubectl plugin</a>.</li><li><a href="https://cloud.google.com/storage/docs/access-control/signed-urls" target="_blank" rel="noopener noreferrer">Signed URLs</a> allow for users to upload their local directory to a bucket without requiring cloud credentials (Similar to how popular consumer clouds function).</li><li>Kubernetes <a href="https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/" target="_blank" rel="noopener noreferrer">port-forwarding</a> allows for serving remote notebooks without requiring admins to deal with networking / TLS concerns. It also leans on existing Kubernetes RBAC for access control.</li></ol><p>Some interesting details:</p><ul><li>Builds are executed remotely for two reasons:<ul><li>Users don't need to install docker.</li><li>It avoids pushing massive container images from one's local machine (pip installs often inflate the final docker image to be much larger than the build context itself).</li></ul></li><li>The client requests an upload URL by specifying the MD5 hash it wishes to upload - allowing for server-side signature verification.</li><li>Builds are skipped entirely if the MD5 hash of the build context already exists in the bucket.</li></ul><p>The system underneath the <code>notebook</code> command:</p><p><img loading="lazy" alt="diagram" src="/assets/images/kubectl-notebook.excalidraw-c10c014aec5fc834c0c9f5c2d4ee10d0.png" width="773" height="1536" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="more-to-come">More to come!<a href="#more-to-come" class="hash-link" aria-label="Direct link to More to come!" title="Direct link to More to come!">​</a></h2><p>Lazy-loading large models from disk...
Incremental dataset loading...
Stay tuned to learn more about how Notebooks on Substratus can speed up your ML workflows.</p><p>Don't forget to star and follow the repo!</p><p><a href="https://github.com/substratusai/substratus" target="_blank" rel="noopener noreferrer">https://github.com/substratusai/substratus</a></p><span><a href="https://github.com/substratusai/substratus" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star substratusai/substratus on GitHub">Star</a></span>]]></content:encoded>
            <category>introduction</category>
            <category>feature</category>
        </item>
        <item>
            <title><![CDATA[Tutorial: Llama2 70b serving on GKE]]></title>
            <link>https://www.substratus.ai/blog/tutorial-llama2-70b-serving-gke</link>
            <guid>https://www.substratus.ai/blog/tutorial-llama2-70b-serving-gke</guid>
            <pubDate>Sun, 06 Aug 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Llama 2 70b is the newest iteration of the Llama model published by Meta, sporting 7 Billion parameters.]]></description>
            <content:encoded><![CDATA[<p>Llama 2 70b is the newest iteration of the Llama model published by Meta, sporting 7 Billion parameters.
Follow along in this tutorial to get Llama 2 70b deployed on GKE:</p><ol><li>Create a GKE cluster with Substratus installed.</li><li>Load the Llama 2 70b model from HuggingFace.</li><li>Serve the model via an interactive inference server.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="install-substratus-on-gcp">Install Substratus on GCP<a href="#install-substratus-on-gcp" class="hash-link" aria-label="Direct link to Install Substratus on GCP" title="Direct link to Install Substratus on GCP">​</a></h2><p>Use the <a href="/docs/installation/gcp">Installation Guide for GCP</a> to install Substratus.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="load-the-model-into-substratus">Load the Model into Substratus<a href="#load-the-model-into-substratus" class="hash-link" aria-label="Direct link to Load the Model into Substratus" title="Direct link to Load the Model into Substratus">​</a></h2><p>You will need to agree to HuggingFace's terms before you can use the Llama 2 model. This means you will need to pass your HuggingFace token to Substratus.</p><p>Let's tell Substratus how to import Llama 2 by defining a Model resource. Create a file named <code>base-model.yaml</code> with the following content:</p><div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token key atrule" style="color:#00a4db">apiVersion</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> substratus.ai/v1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">kind</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> Model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">metadata</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> llama</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">2</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">70b</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">spec</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">image</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> substratusai/model</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">loader</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">huggingface</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">env</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># You would first have to create a secret named `ai` that</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># has the key `HUGGING_FACE_HUB_TOKEN` set to your token.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># E.g. create the secret by running:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># kubectl create secret generic ai --from-literal="HUGGING_FACE_HUB_TOKEN=&lt;my-token&gt;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">HUGGING_FACE_HUB_TOKEN</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> $</span><span class="token punctuation" style="color:#393A34">{</span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"> secrets.ai.HUGGING_FACE_HUB_TOKEN </span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">params</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> meta</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">llama/Llama</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">2</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">70b</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">hf</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Get your HuggingFace token by going to <a href="https://huggingface.co/settings/tokens" target="_blank" rel="noopener noreferrer">HuggingFace Settings &gt; Access Tokens</a>.</p><p>Create a secret with your HuggingFace token:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl create secret generic ai --from-literal</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">"HUGGING_FACE_HUB_TOKEN</span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">my-token</span><span class="token operator" style="color:#393A34">&gt;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Make sure to replace <code>&lt;my-token&gt;</code> with your actual token.</p><p>Run the following command to load the base model:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl apply -f base-model.yaml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Watch Substratus kick off your importing Job.</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get </span><span class="token function" style="color:#d73a49">jobs</span><span class="token plain"> -w</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>You can view the Job logs by running:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl logs -f jobs/llama-2-70b-modeller</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="serve-the-loaded-model">Serve the Loaded Model<a href="#serve-the-loaded-model" class="hash-link" aria-label="Direct link to Serve the Loaded Model" title="Direct link to Serve the Loaded Model">​</a></h2><p>While the Model is loading, we can define our inference server. Create a file named <code>server.yaml</code> with the following content:</p><div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token key atrule" style="color:#00a4db">apiVersion</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> substratus.ai/v1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">kind</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> Server</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">metadata</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> llama</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">2</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">70b</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">spec</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">image</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> substratusai/model</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">server</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">basaran</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">model</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> llama</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">2</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">70b</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">env</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">MODEL_LOAD_IN_4BIT</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">"true"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">resources</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">gpu</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token key atrule" style="color:#00a4db">type</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> nvidia</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">a100</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token key atrule" style="color:#00a4db">count</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Create the Server by running:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl apply -f server.yaml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Once the Model is loaded (marked as <code>ready</code>), Substratus will automatically launch the server. View the state of both resources using kubectl:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get models,servers</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>To view more information about either the Model or Server, you can use <code>kubectl describe</code>:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl describe -f base-model.yaml</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># OR</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl describe -f server.yaml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Once the model is loaded, the initial server startup time is about 20 minutes.
This is because the model is 100GB+ in size and takes a while to load
into GPU memory.</p><p>Look for a log message that the container
is serving at port <code>8080</code>. You can check the logs
by running:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl logs deployment/llama-2-70b-server</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>For demo purposes, you can use port forwarding once the Server is ready on port 8080. Run the following command to forward the container port 8080 to your localhost port 8080:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl port-forward service/llama-2-70b-server </span><span class="token number" style="color:#36acaa">8080</span><span class="token plain">:8080</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Interact with Llama 2 in your browser:
<a href="http://localhost:8080" target="_blank" rel="noopener noreferrer">http://localhost:8080</a></p><p><em>You have now deployed Llama 2 70b!</em></p><p>You can repeat these steps for other models. For example, you
could instead deploy the "Instruct" variation of Llama.</p><p>Stay tuned for another blog post on how to fine-tune Llama 2 70b on your own data.</p>]]></content:encoded>
            <category>tutorial</category>
        </item>
        <item>
            <title><![CDATA[Introducing Substratus]]></title>
            <link>https://www.substratus.ai/blog/introducing-substratus</link>
            <guid>https://www.substratus.ai/blog/introducing-substratus</guid>
            <pubDate>Thu, 03 Aug 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[We are excited to introduce Substratus, the open-source cross-cloud substrate]]></description>
            <content:encoded><![CDATA[<p>We are excited to introduce Substratus, the open-source cross-cloud substrate
for training and serving ML models with an initial focus on Large Language
Models. Fine-tune and serve LLMs on your Kubernetes clusters in your cloud.</p><p>Can’t wait? - Get started with our <a href="/docs/category/quickstart">quick start
docs</a> or jump over to the <a href="https://github.com/substratusai/substratus" target="_blank" rel="noopener noreferrer">GitHub
repo</a>.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="why-substratus"><strong>Why Substratus?</strong><a href="#why-substratus" class="hash-link" aria-label="Direct link to why-substratus" title="Direct link to why-substratus">​</a></h2><p><strong>Press the fast-button for ML</strong>: Leverage out of the box container images to
load a base model, optionally fine-tune with your own dataset and spin up a
model server, all without writing any code.</p><p><strong>Notebook-integrated workflows:</strong> Launch a remote, containerized, GPU-enabled
notebook from a local directory with a single command. Develop in the exact same
environment as your long running training jobs.</p><p><strong>No vendor lock-in</strong>: Substratus is open-source and can run anywhere Kubernetes
runs.</p><p><strong>Keep company data internal</strong>: Deploy in your cloud account. Training data and
inference APIs stay within your company’s network.</p><p><strong>Best practices by default:</strong> Substratus models are immutable and contain
information about their lineage. Datasets are imported and snapshotted using
off-the-shelf manifests. Training executes in containerized environments, using
immutable base artifacts. Inference servers are pre-configured to leverage
quantization on supported models. GitOps is built-in, not bolted-on.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="guiding-principles"><strong>Guiding Principles</strong><a href="#guiding-principles" class="hash-link" aria-label="Direct link to guiding-principles" title="Direct link to guiding-principles">​</a></h2><p>As we continue to develop Substratus, we’re grounded in the following guiding
principles:</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-prioritize-simplicity"><strong>1. Prioritize Simplicity</strong><a href="#1-prioritize-simplicity" class="hash-link" aria-label="Direct link to 1-prioritize-simplicity" title="Direct link to 1-prioritize-simplicity">​</a></h3><p> We believe the importance of minimizing complexity in software cannot be
understated. In Substratus, we will work hard to keep complexity to a minimum
as the project grows. The Substratus API currently consists of 4 resource
types: Datasets, Models, Servers, and Notebooks. The project currently depends
on two cloud services outside of the cluster: a bucket and a container registry
(we are working on making these optional too). The project does not (and will
never) depend on a web of complex components like Istio.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-prioritize-ux"><strong>2. Prioritize UX</strong><a href="#2-prioritize-ux" class="hash-link" aria-label="Direct link to 2-prioritize-ux" title="Direct link to 2-prioritize-ux">​</a></h3><p>We believe a company’s most precious resource is their engineer’s time.
Substratus seeks to maximize the productivity of data scientists and engineers
through providing a best-in-class user experience. We strive to build a set of
well-designed primitives that allow ML practitioners to enter a flow state as
they move between importing data, training, and serving models.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="roadmap"><strong>Roadmap</strong><a href="#roadmap" class="hash-link" aria-label="Direct link to roadmap" title="Direct link to roadmap">​</a></h2><p>We are fast at work adding new functionality, focused on creating the most
productive and enjoyable platform for ML practitioners. Coming soon:</p><ol><li>Support for AWS and Azure</li><li>VS Code Notebook Integration</li><li>Large-scale distributed training</li><li>ML ecosystem integrations</li></ol><p>Try Substratus today in your GCP project by following the <a href="/docs/category/quickstart">quick start
docs</a>. Let us know what features you
would like to see on our <a href="https://github.com/substratusai/substratus" target="_blank" rel="noopener noreferrer">GitHub
repo</a> and don’t forget to add a
star!</p>]]></content:encoded>
            <category>hello world</category>
            <category>introduction</category>
            <category>oss launch</category>
        </item>
    </channel>
</rss>