---
slug: llama2-70b
title: "Tutorial: Llama2 70b on GKE"
authors:
- name: Sam Stoelinga
  title: Engineer
  url: https://github.com/samos123
  image_url: https://avatars.githubusercontent.com/u/388784?v=4

tags: [tutorial]
---

Llama 2 70b, is the newest iteration of the Llama model published by Meta.
Follow along in this tutorial to get Llama 2 70b deployed on GKE:

1. Install Substratus on GKE
2. Load the Llama 2 70b model
3. Serve Llama 2 70b 

## Install Substratus on GCP
Use the Substratus installer to create a new GKE cluster, GCS bucket
and Artifact Registry:

```bash
docker run -it \
  -v ${HOME}/.kube:/root/.kube \
  -e PROJECT=$(gcloud config get project) \
  -e TOKEN=$(gcloud auth print-access-token) \
  substratusai/installer:latest gcp-up.sh
```

## Load the Llama 2 70b model
You need to agree to HuggingFace's terms before you can use the Llama 2 model. This means you can't simply download the model without logging into HuggingFace. But don't worry, the Substratus model-loader-huggingface can use your HuggingFace token.

Let's get started with creating the Substrats `Model` resource. Create a file named `base-model.yaml` with the following content:
```yaml
apiVersion: substratus.ai/v1
kind: Model
metadata:
  name: llama-2-70b
spec:
  image: substratusai/model-loader-huggingface
  params:
    name: meta-llama/Llama-2-70b-hf
    hugging_face_hub_token: ${HUGGINGFACE_TOKEN}
```
Notice this part `${HUGGINGFACE_TOKEN}` in the `base-model.yaml` file.

Get your HuggingFace token by going to [HuggingFace Settings > Access Tokens](
    https://huggingface.co/settings/tokens
).
Create an environment variable that holds your HuggingFace token:
```bash
export HUGGINGFACE_TOKEN=replace_me
```

Let's use `envsubst` to add our actual token since
and apply the manifest. Run the following command:
```bash
cat base-model.yaml | envsubst | kubectl apply -f -
```

You can watch the progress by running:
```bash
kubectl logs -f jobs/llama-2-70b-modeller
```

Wait till the model reports being ready:
```bash
kubectl describe model llama-2-70b
```

## Serve the loaded Llama 2 70b model
Once the model is loaded you can create a Substratus Server
resource to serve the model.

Create a file named `server.yaml` with the following content:
```yaml
apiVersion: substratus.ai/v1
kind: Server
metadata:
  name: llama-2-70b
spec:
  image: substratusai/model-server-basaran
  model:
    name: llama-2-70b
  resources:
    gpu:
      type: nvidia-a100
      count: 2
```

Create the server by running:
```bash
kubectl apply -f server.yaml
```

The initial startup time is about 20 minutes.
This is because the model is 100GB+ in size and it
needs to load the data from GCS into GPU memory.

Wait until you see a log message that the container
is serving at port `8080`. You can check the logs
by running:
```bash
kubectl logs deployment/llama-2-70b-server
```

You can then use port forwarding once the Server is ready on port 8080. Run the following command to forward the container port 8080 to your localhost port 8080:
```bash
kubectl port-forward service/llama-2-70b-server 8080:8080
```

In your browser you can now open the following URL:
[http://localhost:8080](http://localhost:8080)

You have now deployed the Llama 2 70b base model. You
can repeat the steps for other models, for example, you
could instead deploy llama-2-70b-instruct-v2.

## Cleanup
Run the following command to delete all the Substratus created resources:
```bash
docker run -it \
  -v ${HOME}/.kube:/root/.kube \
  -e PROJECT=$(gcloud config get project) \
  -e TOKEN=$(gcloud auth print-access-token) \
  substratusai/installer:latest gcp-down.sh
```