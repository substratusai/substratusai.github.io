---
slug: tutorial-llama2-70b-serving-gke
title: "Tutorial: Llama2 70b serving on GKE"
authors:
- name: Sam Stoelinga
  title: Engineer
  url: https://github.com/samos123
  image_url: https://avatars.githubusercontent.com/u/388784?v=4

tags: [tutorial]
---

Llama 2 70b is the newest iteration of the Llama model published by Meta, sporting 7 Billion parameters.
Follow along in this tutorial to get Llama 2 70b deployed on GKE:

1. Create a GKE cluster with Substratus installed.
2. Load the Llama 2 70b model from HuggingFace.
3. Serve the model via an interactive inference server.

## Install Substratus on GCP
Use the Substratus installer to create a new GKE cluster with supporting infrastructure (a GCS bucket
and Artifact Registry):

```bash
docker run -it \
  -v ${HOME}/.kube:/root/.kube \
  -e PROJECT=$(gcloud config get project) \
  -e TOKEN=$(gcloud auth print-access-token) \
  substratusai/installer:v0.8.0 gcp-up.sh
```

## Load the Model into Substratus
You will need to agree to HuggingFace's terms before you can use the Llama 2 model. This means you will need to pass your HuggingFace token to Substratus.

Let's tell Substratus how to import Llama 2 by defining a Model resource. Create a file named `base-model.yaml` with the following content:
```yaml
apiVersion: substratus.ai/v1
kind: Model
metadata:
  name: llama-2-70b
spec:
  image: substratusai/model-loader-huggingface
  params:
    name: meta-llama/Llama-2-70b-hf
    hugging_face_hub_token: ${HUGGINGFACE_TOKEN}
```
Notice the `${HUGGINGFACE_TOKEN}` placeholder in the `base-model.yaml` file.

Get your HuggingFace token by going to [HuggingFace Settings > Access Tokens](
    https://huggingface.co/settings/tokens
).
Create an environment variable that holds your HuggingFace token:
```bash
export HUGGINGFACE_TOKEN=replace_me
```

Let's use `envsubst` to set the `${HUGGINGFACE_TOKEN}` variable when we apply the Model.

Run the following command:
```bash
cat base-model.yaml | envsubst | kubectl apply -f -
```

Watch Substratus kick off your importing Job.

```bash
kubectl get jobs -w
```

You can view the Job logs by running:
```bash
kubectl logs -f jobs/llama-2-70b-modeller
```

## Serve the Loaded Model
While the Model is loading, we can define our inference server. Create a file named `server.yaml` with the following content:
```yaml
apiVersion: substratus.ai/v1
kind: Server
metadata:
  name: llama-2-70b
spec:
  image: substratusai/model-server-basaran
  model:
    name: llama-2-70b
  resources:
    gpu:
      type: nvidia-a100
      count: 2
```

Create the Server by running:
```bash
kubectl apply -f server.yaml
```

Once the Model is loaded (marked as `ready`), Substratus will automatically launch the server. View the state of both resources using kubectl:

```bash
kubectl get models,servers
```

To view more information about either the Model or Server, you can use `kubectl describe`:

```bash
kubectl describe -f base-model.yaml
# OR
kubectl describe -f server.yaml
```

Once the model is loaded, the initial server startup time is about 20 minutes.
This is because the model is 100GB+ in size and takes a while to load
into GPU memory.

Look for a log message that the container
is serving at port `8080`. You can check the logs
by running:
```bash
kubectl logs deployment/llama-2-70b-server
```

For demo purposes, you can use port forwarding once the Server is ready on port 8080. Run the following command to forward the container port 8080 to your localhost port 8080:
```bash
kubectl port-forward service/llama-2-70b-server 8080:8080
```

Interact with Llama 2 in your browser:
[http://localhost:8080](http://localhost:8080)

*You have now deployed Llama 2 70b!*

You can repeat these steps for other models. For example, you
could instead deploy the "Instruct" variation of Llama.

Stay tuned for another blog post on how to fine-tune Llama 2 70b on your own data.

## Cleanup
Run the following command to delete all resources:
```bash
docker run -it \
  -v ${HOME}/.kube:/root/.kube \
  -e PROJECT=$(gcloud config get project) \
  -e TOKEN=$(gcloud auth print-access-token) \
  substratusai/installer:v0.8.0 gcp-down.sh
```
