<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://www.substratus.ai/blog</id>
    <title>Substratus Blog</title>
    <updated>2023-11-16T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://www.substratus.ai/blog"/>
    <subtitle>Substratus Blog</subtitle>
    <icon>https://www.substratus.ai/img/favicon.ico</icon>
    <entry>
        <title type="html"><![CDATA[Calculating GPU memory for serving LLMs]]></title>
        <id>https://www.substratus.ai/blog/calculating-gpu-memory-for-llm</id>
        <link href="https://www.substratus.ai/blog/calculating-gpu-memory-for-llm"/>
        <updated>2023-11-16T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[How many GPUs do I need to be able to serve Llama 70B? In order]]></summary>
        <content type="html"><![CDATA[<p>How many GPUs do I need to be able to serve Llama 70B? In order
to answer that, you need to know how much GPU memory will be required by
the Large Language Model.</p>
<p>The formula is simple:</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>M</mi><mo>=</mo><mfrac><mrow><mo stretchy="false">(</mo><mi>P</mi><mo>∗</mo><mn>4</mn><mi>B</mi><mo stretchy="false">)</mo></mrow><mrow><mo stretchy="false">(</mo><mn>32</mn><mi mathvariant="normal">/</mi><mi>Q</mi><mo stretchy="false">)</mo></mrow></mfrac><mo>∗</mo><mn>1.2</mn></mrow><annotation encoding="application/x-tex">M = \dfrac{(P * 4B)}{ (32  / Q)} * 1.2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.363em;vertical-align:-0.936em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mopen">(</span><span class="mord">32/</span><span class="mord mathnormal">Q</span><span class="mclose">)</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord">4</span><span class="mord mathnormal" style="margin-right:0.05017em">B</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1.2</span></span></span></span></span>
<table><thead><tr><th>Symbol</th><th>Description</th></tr></thead><tbody><tr><td>M</td><td>GPU memory expressed in Gigabyte</td></tr><tr><td>P</td><td>The amount of parameters in the model. E.g. a 7B model has 7 billion parameters.</td></tr><tr><td>4B</td><td>4 bytes, expressing the bytes used for each parameter</td></tr><tr><td>32</td><td>There are 32 bits in 4 bytes</td></tr><tr><td>Q</td><td>The amount of bits that should be used for loading the model. E.g. 16 bits, 8 bits or 4 bits.</td></tr><tr><td>1.2</td><td>Represents a 20% overhead of loading additional things in GPU memory.</td></tr></tbody></table>
<p>Now let's try out some examples.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="gpu-memory-required-for-serving-llama-70b">GPU memory required for serving Llama 70B<a href="https://www.substratus.ai/blog/calculating-gpu-memory-for-llm#gpu-memory-required-for-serving-llama-70b" class="hash-link" aria-label="Direct link to GPU memory required for serving Llama 70B" title="Direct link to GPU memory required for serving Llama 70B">​</a></h3>
<p>Let's try it out for Llama 70B that we will load in 16 bit.
The model has 70 billion parameters.</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mn>70</mn><mo>∗</mo><mn>4</mn><mrow><mi mathvariant="normal">b</mi><mi mathvariant="normal">y</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">s</mi></mrow></mrow><mrow><mn>32</mn><mi mathvariant="normal">/</mi><mn>16</mn></mrow></mfrac><mo>∗</mo><mn>1.2</mn><mo>=</mo><mn>168</mn><mrow><mi mathvariant="normal">G</mi><mi mathvariant="normal">B</mi></mrow></mrow><annotation encoding="application/x-tex">\dfrac{70  * 4 \mathrm{bytes}}{32 / 16} * 1.2 = 168\mathrm{GB}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.3074em;vertical-align:-0.936em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">32/16</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">70</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord">4</span><span class="mord"><span class="mord mathrm">bytes</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1.2</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord">168</span><span class="mord"><span class="mord mathrm">GB</span></span></span></span></span></span>
<p>That's quite a lot of memory. A single A100 80GB wouldn't be enough, although
2x A100 80GB should be enough to serve the Llama 2 70B model in 16 bit mode.</p>
<p><strong>How to further reduce GPU memory required for Llama 2 70B?</strong></p>
<p>Quantization is a method to reduce the memory footprint. Quantization is able to do this by reducing the precision of the model's parameters from floating-point to lower-bit representations, such as 8-bit integers. This process significantly decreases the memory and computational requirements, enabling more efficient deployment of the model, particularly on devices with limited resources. However, it requires careful management to maintain the model's performance, as reducing precision can potentially impact the accuracy of the outputs.</p>
<p>In general, the consensus seems to be that 8 bit quantization achieves similar performance to using 16 bit. However, 4 bit quantization could have a noticeable impact to the model performance.</p>
<p>Let's do another example where we use <strong>4 bit quantization of Llama 2 70B</strong>:</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mn>70</mn><mo>∗</mo><mn>4</mn><mrow><mi mathvariant="normal">b</mi><mi mathvariant="normal">y</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">s</mi></mrow></mrow><mrow><mn>32</mn><mi mathvariant="normal">/</mi><mn>4</mn></mrow></mfrac><mo>∗</mo><mn>1.2</mn><mo>=</mo><mn>42</mn><mrow><mi mathvariant="normal">G</mi><mi mathvariant="normal">B</mi></mrow></mrow><annotation encoding="application/x-tex">\dfrac{70  * 4 \mathrm{bytes}}{32 / 4} * 1.2 = 42\mathrm{GB}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.3074em;vertical-align:-0.936em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">32/4</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">70</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord">4</span><span class="mord"><span class="mord mathrm">bytes</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1.2</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord">42</span><span class="mord"><span class="mord mathrm">GB</span></span></span></span></span></span>
<p>This is something you could run on 2 x L4 24GB GPUs.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="relevant-tools-and-resources">Relevant tools and resources<a href="https://www.substratus.ai/blog/calculating-gpu-memory-for-llm#relevant-tools-and-resources" class="hash-link" aria-label="Direct link to Relevant tools and resources" title="Direct link to Relevant tools and resources">​</a></h3>
<ol>
<li><a href="https://huggingface.co/spaces/Vokturz/can-it-run-llm" target="_blank" rel="noopener noreferrer">Tool for checking how many GPUs you need for a specific model</a></li>
<li><a href="https://blog.eleuther.ai/transformer-math/" target="_blank" rel="noopener noreferrer">Transformer Math 101</a></li>
</ol>
<p>Got more questions? Don't hesitate to join our Discord and ask away.</p>
<a href="https://discord.gg/JeXhcmjZVm"><img alt="discord-invite" src="https://dcbadge.vercel.app/api/server/JeXhcmjZVm?style=flat"></a>]]></content>
        <author>
            <name>Sam Stoelinga</name>
            <uri>https://github.com/samos123</uri>
        </author>
        <category label="llm" term="llm"/>
        <category label="gpu" term="gpu"/>
        <category label="memory" term="memory"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deploying Mistral 7B Instruct on K8s using TGI]]></title>
        <id>https://www.substratus.ai/blog/mistral-7b-instruct-k8s-helm-tgi</id>
        <link href="https://www.substratus.ai/blog/mistral-7b-instruct-k8s-helm-tgi"/>
        <updated>2023-10-21T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Learn how to use the text-generation-inference (TGI) Helm Chart to quickly]]></summary>
        <content type="html"><![CDATA[<img src="https://www.substratus.ai/img/mistral-7b-helm-k8s-tgi.png" alt="mistral 7b k8s helm" width="100%">
<p>Learn how to use the text-generation-inference (TGI) Helm Chart to quickly
deploy Mistral 7B Instruct on your K8s cluster.</p>
<p>Add the Substratus.ai Helm repo:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">helm repo add substratusai https://substratusai.github.io/helm</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>This command adds a new Helm repository, making the <code>text-generation-inference</code>
Helm chart available for installation.</p>
<p>Create a configuration file named <code>values.yaml</code>. This file will contain the necessary settings for your deployment. Here’s an example of what the content should look like:</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">model: mistralai/Mistral-7B-Instruct-v0.1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># resources: # optional, override if you need more than 1 GPU</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">#   limits:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">#     nvidia.com/gpu: 1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># nodeSelector: # optional, can be used to target specific GPUs</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">#   cloud.google.com/gke-accelerator: nvidia-l4</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>In this configuration file, you are specifying the model to be deployed and optionally setting resource limits or targeting specific nodes based on your requirements.</p>
<p>With your configuration file ready, you can now deploy Mistral 7B Instruct using Helm:</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">helm install mistral-7b-instruct substratusai/text-generation-inference \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    -f values.yaml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>This command initiates the deployment, creating a Kubernetes Deployment and Service based on the settings defined in your <code>values.yaml</code> file.</p>
<p>After initiating the deployment, it's important to ensure that everything is running as expected. Run the following command to get detailed information about the newly created pod:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl describe pod -l app.kubernetes.io/instance=mistral-7b-instruct</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>This will display various details about the pod, helping you to confirm that it has been successfully created and is in the right state. Note that depending on your cluster's setup, you might need to wait for the cluster autoscaler to provision additional resources if necessary.</p>
<p>Once the pod is running, check the logs to ensure that the model is initializing properly:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl logs -f -l app.kubernetes.io/instance=mistral-7b-instruct</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>The model first downloads the model and after a few minutes, you should
see a message that looks like this:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Invalid hostname, defaulting to 0.0.0.0</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>This is expected and means it's now serving on host 0.0.0.0.</p>
<p>By default, the model is only accessible within the Kubernetes cluster. To access it from your local machine, set up a port forward:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl port-forward deployments/mistral-7b-instruct-text-generation-inference 8080:8080</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>This command maps port 8080 on your local machine to port 8080 on the deployed pod, allowing you to interact with the model directly.</p>
<p>With the service exposed, you can now run inference tasks. To explore the available API endpoints and their usage, visit the TGI API documentation at <a href="http://localhost:8080/docs" target="_blank" rel="noopener noreferrer">http://localhost:8080/docs</a>.</p>
<p>Here’s an example of how to use curl to run an inference task:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">curl 127.0.0.1:8080/generate -X POST \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    -H 'Content-Type: application/json' \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --data-binary @- &lt;&lt; 'EOF' | jq -r '.generated_text'</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">{</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    "inputs": "&lt;s&gt;[INST] Write a K8s YAML file to create a pod that deploys nginx[/INST]",</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    "parameters": {"max_new_tokens": 400}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">EOF</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>In this example, we are instructing the model to generate a Kubernetes YAML file for deploying an Nginx pod. The prompt includes specific tokens that the Mistral 7B Instruct model recognizes, ensuring accurate and context-aware responses.</p>
<p>The prompt we are using starts with <code>&lt;s&gt;</code> token which indicates beginning of a sequence.
The <code>[INST]</code> token tells Mistral-7b Instruct what follows is an instruction. The Mistral 7B
Instruct model was finetuned with this prompt template, so it's important to re-use that
same prompt template.</p>
<p>The response is quite impressive, it did return a valid K8s YAML manifest
and also instructions on how to apply it.</p>
<p>Need help? Want to see other models? other serving frameworks?<br>
<!-- -->Join our Discord and ask me directly:</p>
<a href="https://discord.gg/JeXhcmjZVm"><img alt="discord-invite" src="https://dcbadge.vercel.app/api/server/JeXhcmjZVm?style=flat"></a>]]></content>
        <author>
            <name>Sam Stoelinga</name>
            <uri>https://github.com/samos123</uri>
        </author>
        <category label="k8s" term="k8s"/>
        <category label="mistral-7b" term="mistral-7b"/>
        <category label="helm" term="helm"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[The K8s YAML dataset]]></title>
        <id>https://www.substratus.ai/blog/k8s-yaml-dataset</id>
        <link href="https://www.substratus.ai/blog/k8s-yaml-dataset"/>
        <updated>2023-10-09T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Excited to announce the K8s YAML dataset containing]]></summary>
        <content type="html"><![CDATA[<img src="https://www.substratus.ai/img/the-stack-yaml-k8s-banner.png" alt="kubectl notebook" width="100%">
<p>Excited to announce the K8s YAML dataset containing
276,520 valid K8s YAML files.</p>
<p>HuggingFace Dataset: <a href="https://huggingface.co/datasets/substratusai/the-stack-yaml-k8s" target="_blank" rel="noopener noreferrer">https://huggingface.co/datasets/substratusai/the-stack-yaml-k8s</a><br>
<!-- -->Source code: <a href="https://github.com/substratusai/the-stack-yaml-k8s" target="_blank" rel="noopener noreferrer">https://github.com/substratusai/the-stack-yaml-k8s</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="why">Why?<a href="https://www.substratus.ai/blog/k8s-yaml-dataset#why" class="hash-link" aria-label="Direct link to Why?" title="Direct link to Why?">​</a></h2>
<ul>
<li>This dataset can be used to fine-tune an LLM directly</li>
<li>New datasets can be created from his dataset such as an K8s instruct dataset (coming soon!)</li>
<li>What's your use case?</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how">How?<a href="https://www.substratus.ai/blog/k8s-yaml-dataset#how" class="hash-link" aria-label="Direct link to How?" title="Direct link to How?">​</a></h2>
<p>Getting a lot of K8s YAML manifests wasn't easy. My initial approach
was to use the Kubernetes website and scrape the YAML example files,
however the issue was the quantity since I could only scrape
about ~250 YAML examples that way.</p>
<p>Luckily, I came across <a href="https://huggingface.co/datasets/bigcode/the-stack" target="_blank" rel="noopener noreferrer">the-stack</a> dataset
which is a cleaned dataset of code on GitHub. The dataset is nicely structured by language
and I noticed that <code>yaml</code> was one of the languages in the dataset.</p>
<p>Install libraries used in this blog post:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip3 install datasets kubernetes-validate</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Let's load the <code>the-stack</code> dataset but only the YAML files (takes about 200GB of disk space):</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> datasets </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> load_dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">ds </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> load_dataset</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">"bigcode/the-stack"</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> data_dir</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">"data/yaml"</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> split</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">"train"</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Once loaded there are 13,439,939 YAML files in <code>ds</code>.</p>
<p>You can check the content of one of the files:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">print</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">ds</span><span class="token punctuation" style="color:#393A34">[</span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">"content"</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>You probably notice that this ain't a K8s YAML file, so next we need to filter
these 13 million YAML files and only keep the one that have valid K8 YAML.</p>
<p>The approach I took was to use the <a href="https://github.com/willthames/kubernetes-validate" target="_blank" rel="noopener noreferrer">kubernetes-validate</a> OSS library. It turned out
that YAML parsing was too slow so I added a 10x speed improvement
by eagerly checking if "Kind or "kind" is not a substring in the YAML file.</p>
<p>Here is the validate function that takes the yaml_content as a string and
returns if the content was valid K8s YAML or not:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> kubernetes_validate</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> yaml</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">validate</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">yaml_content</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token builtin">str</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">try</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Speed optimization to return early without having to load YAML</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">"kind"</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">not</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> yaml_content </span><span class="token keyword" style="color:#00009f">and</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">"Kind"</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">not</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> yaml_content</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">False</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        data </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> yaml</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">safe_load</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">yaml_content</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        kubernetes_validate</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">validate</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">data</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">'1.22'</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> strict</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">True</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">except</span><span class="token plain"> Exception </span><span class="token keyword" style="color:#00009f">as</span><span class="token plain"> e</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">False</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">validate</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">ds</span><span class="token punctuation" style="color:#393A34">[</span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">"content"</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Now all that's needed is to filter out all YAML files that aren't valid:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> os</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">os</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">cpu_count</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">valid_k8s </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> ds</span><span class="token punctuation" style="color:#393A34">.</span><span class="token builtin">filter</span><span class="token punctuation" style="color:#393A34">(</span><span class="token keyword" style="color:#00009f">lambda</span><span class="token plain"> batch</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">validate</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> x </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> batch</span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">"content"</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                      num_proc</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">os</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">cpu_count</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> batched</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>There were 276,520 YAML files left in <code>valid_k8s</code>. You can print one again to see:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">print</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">valid_k8s</span><span class="token punctuation" style="color:#393A34">[</span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">"content"</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>You can upload the dataset back to HuggingFace by running:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">valid_k8s</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">push_to_hub</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">"substratusai/the-stack-yaml-k8s"</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="whats-next">What's next?<a href="https://www.substratus.ai/blog/k8s-yaml-dataset#whats-next" class="hash-link" aria-label="Direct link to What's next?" title="Direct link to What's next?">​</a></h2>
<p>Creating a new dataset called K8s Instruct that also provides a prompt for each YAML file.</p>
<p>Support the project by adding a star on GitHub! ❤️
<span><a href="https://github.com/substratusai/substratus" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star substratusai/substratus on GitHub">Star</a></span></p>]]></content>
        <author>
            <name>Sam Stoelinga</name>
            <uri>https://github.com/samos123</uri>
        </author>
        <category label="k8s" term="k8s"/>
        <category label="yaml" term="yaml"/>
        <category label="dataset" term="dataset"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tutorial: K8s Kind with GPUs]]></title>
        <id>https://www.substratus.ai/blog/kind-with-gpus</id>
        <link href="https://www.substratus.ai/blog/kind-with-gpus"/>
        <updated>2023-09-07T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Don't you just love it when you submit a PR and it turns out that no code is]]></summary>
        <content type="html"><![CDATA[<div class="video-container"><iframe class="video" src="https://www.youtube-nocookie.com/embed/O1683vzaJVE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"></iframe></div>
<p>Don't you just love it when you submit a PR and it turns out that no code is
needed? That's exactly what happened when I tried add GPU support to Kind.</p>
<p>In this blog post you will learn how to configure Kind such
that it can use the GPUs on your device. Credit to
<a href="https://github.com/kubernetes-sigs/kind/pull/3257#issuecomment-1607287275" target="_blank" rel="noopener noreferrer">@klueska</a>
for the solution.</p>
<p>Install the NVIDIA container toolkit by following the <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/user-guide.html#adding-the-nvidia-runtime" target="_blank" rel="noopener noreferrer">official install docs</a>.</p>
<p>Configure NVIDIA to be the default runtime for docker:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo nvidia-ctk runtime configure --runtime=docker --set-as-default</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sudo systemctl restart docker</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Set <code>accept-nvidia-visible-devices-as-volume-mounts = true</code> in <code>/etc/nvidia-container-runtime/config.toml</code>:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo sed -i '/accept-nvidia-visible-devices-as-volume-mounts/c\accept-nvidia-visible-devices-as-volume-mounts = true' /etc/nvidia-container-runtime/config.toml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Create a Kind Cluster:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kind create cluster --name substratus --config - &lt;&lt;EOF</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">apiVersion: kind.x-k8s.io/v1alpha4</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kind: Cluster</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">nodes:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- role: control-plane</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  image: kindest/node:v1.27.3@sha256:3966ac761ae0136263ffdb6cfd4db23ef8a83cba8a463690e98317add2c9ba72</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # required for GPU workaround</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  extraMounts:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    - hostPath: /dev/null</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      containerPath: /var/run/nvidia-container-devices/all</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">EOF</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Workaround for issue with missing required file <code>/sbin/ldconfig.real</code>:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"># https://github.com/NVIDIA/nvidia-docker/issues/614#issuecomment-423991632</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker exec -ti substratus-control-plane ln -s /sbin/ldconfig /sbin/ldconfig.real</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Install the K8s NVIDIA GPU operator so K8s is aware of your NVIDIA device:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">helm repo add nvidia https://helm.ngc.nvidia.com/nvidia || true</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">helm repo update</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">helm install --wait --generate-name \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     -n gpu-operator --create-namespace \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     nvidia/gpu-operator --set driver.enabled=false</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>You should now have a working Kind cluster that can access your GPU.
Verify it by running a simple pod:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl apply -f - &lt;&lt; EOF</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">apiVersion: v1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kind: Pod</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">metadata:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  name: cuda-vectoradd</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">spec:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  restartPolicy: OnFailure</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  containers:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  - name: cuda-vectoradd</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    image: "nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda11.7.1-ubuntu20.04"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    resources:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      limits:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        nvidia.com/gpu: 1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">EOF</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>]]></content>
        <author>
            <name>Sam Stoelinga</name>
            <uri>https://github.com/samos123</uri>
        </author>
        <category label="kind" term="kind"/>
        <category label="gpu" term="gpu"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Converting HuggingFace Models to GGUF/GGML]]></title>
        <id>https://www.substratus.ai/blog/converting-hf-model-gguf-model</id>
        <link href="https://www.substratus.ai/blog/converting-hf-model-gguf-model"/>
        <updated>2023-08-31T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Llama.cpp is a great way to run LLMs efficiently on CPUs and GPUs. The downside]]></summary>
        <content type="html"><![CDATA[<p>Llama.cpp is a great way to run LLMs efficiently on CPUs and GPUs. The downside
however is that you need to convert models to a format that's supported by Llama.cpp,
which is now the GGUF file format.  In this blog post you will learn how to convert
a HuggingFace model (Vicuna 13b v1.5) to GGUF model.</p>
<p>At the time of writing, Llama.cpp supports
the following models:</p>
<ul>
<li>LLaMA 🦙</li>
<li>LLaMA 2 🦙🦙</li>
<li>Falcon</li>
<li>Alpaca</li>
<li>GPT4All</li>
<li>Chinese LLaMA / Alpaca and Chinese LLaMA-2 / Alpaca-2</li>
<li>Vigogne (French)</li>
<li>Vicuna</li>
<li>Koala</li>
<li>OpenBuddy 🐶 (Multilingual)</li>
<li>Pygmalion 7B / Metharme 7B</li>
<li>WizardLM</li>
<li>Baichuan-7B and its derivations (such as baichuan-7b-sft)</li>
<li>Aquila-7B / AquilaChat-7B</li>
</ul>
<p>At a high-level you will be going through the following steps:</p>
<ul>
<li>Downloading a HuggingFace model</li>
<li>Running llama.cpp <code>convert.py</code> on the HuggingFace model</li>
<li>(Optionally) Uploading the model back to HuggingFace</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="downloading-a-huggingface-model">Downloading a HuggingFace model<a href="https://www.substratus.ai/blog/converting-hf-model-gguf-model#downloading-a-huggingface-model" class="hash-link" aria-label="Direct link to Downloading a HuggingFace model" title="Direct link to Downloading a HuggingFace model">​</a></h3>
<p>There are various ways to download models, but in my experience the <code>huggingface_hub</code>
library has been the most reliable. The <code>git clone</code> method occasionally results in
OOM errors for large models.</p>
<p>Install the <code>huggingface_hub</code> library:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install huggingface_hub</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Create a Python script named <code>download.py</code> with the following content:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> huggingface_hub </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> snapshot_download</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model_id</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">"lmsys/vicuna-13b-v1.5"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">snapshot_download</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">repo_id</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">model_id</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> local_dir</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">"vicuna-hf"</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                  local_dir_use_symlinks</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">False</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> revision</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">"main"</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Run the Python script:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">python download.py</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>You should now have the model downloaded to a directory called
<code>vicuna-hf</code>. Verify by running:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">ls -lash vicuna-hf</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="converting-the-model">Converting the model<a href="https://www.substratus.ai/blog/converting-hf-model-gguf-model#converting-the-model" class="hash-link" aria-label="Direct link to Converting the model" title="Direct link to Converting the model">​</a></h3>
<p>Now it's time to convert the downloaded HuggingFace model to a GGUF model.
Llama.cpp comes with a converter script to do this.</p>
<p>Get the script by cloning the llama.cpp repo:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">git clone https://github.com/ggerganov/llama.cpp.git</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Install the required python libraries:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install -r llama.cpp/requirements.txt</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Verify the script is there and understand the various options:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">python llama.cpp/convert.py -h</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Convert the HF model to GGUF model:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">python llama.cpp/convert.py vicuna-hf \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --outfile vicuna-13b-v1.5.gguf \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --outtype q8_0</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>In this case we're also quantizing the model to 8 bit by setting
<code>--outtype q8_0</code>. Quantizing helps improve inference speed, but it can
negatively impact quality.
You can use <code>--outtype f16</code> (16 bit) or <code>--outtype f32</code> (32 bit) to preserve original
quality.</p>
<p>Verify the GGUF model was created:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">ls -lash vicuna-13b-v1.5.gguf</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="pushing-the-gguf-model-to-huggingface">Pushing the GGUF model to HuggingFace<a href="https://www.substratus.ai/blog/converting-hf-model-gguf-model#pushing-the-gguf-model-to-huggingface" class="hash-link" aria-label="Direct link to Pushing the GGUF model to HuggingFace" title="Direct link to Pushing the GGUF model to HuggingFace">​</a></h3>
<p>You can optionally push back the GGUF model to HuggingFace.</p>
<p>Create a Python script with the filename <code>upload.py</code> that
has the following content:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> huggingface_hub </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> HfApi</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">api </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> HfApi</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model_id </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">"substratusai/vicuna-13b-v1.5-gguf"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">api</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">create_repo</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">model_id</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> exist_ok</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> repo_type</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">"model"</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">api</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">upload_file</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    path_or_fileobj</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">"vicuna-13b-v1.5.gguf"</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    path_in_repo</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">"vicuna-13b-v1.5.gguf"</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    repo_id</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">model_id</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Get a HuggingFace Token that has write permission from here:
<a href="https://huggingface.co/settings/tokens" target="_blank" rel="noopener noreferrer">https://huggingface.co/settings/tokens</a></p>
<p>Set your HuggingFace token:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">export HUGGING_FACE_HUB_TOKEN=&lt;paste-your-own-token&gt;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Run the <code>upload.py</code> script:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">python upload.py</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Interested in learning how to automate flows like this? Checkout our
open source project:
<span><a href="https://github.com/substratusai/substratus" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star substratusai/substratus on GitHub">Star</a></span></p>]]></content>
        <author>
            <name>Sam Stoelinga</name>
            <uri>https://github.com/samos123</uri>
        </author>
        <category label="llama.cpp" term="llama.cpp"/>
        <category label="gguf" term="gguf"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Kind Local Llama on K8s]]></title>
        <id>https://www.substratus.ai/blog/kind-local-llama-on-rtx-2060</id>
        <link href="https://www.substratus.ai/blog/kind-local-llama-on-rtx-2060"/>
        <updated>2023-08-25T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[A Llama 13B parameter model running on a laptop with a mere RTX 2060?!]]></summary>
        <content type="html"><![CDATA[<img src="https://www.substratus.ai/img/kind-llama-on-laptop-gpu.png" alt="kubectl notebook" width="100%">
<!-- -->
<!-- -->
<p>A Llama 13B parameter model running on a laptop with a mere RTX 2060?!
Yes, it all ran surprisingly well at around 7 tokens / sec.
Follow along and learn how to do this on your environment.</p>
<p>My laptop setup looks like this:</p>
<ul>
<li>Kind for deploying a single node K8s cluster</li>
<li>AMD Ryzen 7 (8 threads), 16 GB system memory, RTX 2060 (6GB GPU memory)</li>
<li>Llama.cpp/GGML for fast serving and loading larger models on consumer hardware</li>
</ul>
<p>You might be wondering: How can a model with 13 billion parameters fit into a 6GB GPU? You'd expect it to need about 13GB, especially if it's running in 4-bit mode, right?
Yes it should because 13 billion * 4 bytes / (32 bits / 4 bits) = 13 GB.
But thanks to <a href="https://github.com/ggerganov/llama.cpp" target="_blank" rel="noopener noreferrer">Llama.cpp</a>, we can load only parts of the model into the GPU. Plus, Llama.cpp can run efficiently just using the CPU.</p>
<p>Want to try this out yourself? Follow a long for a fun ride.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="create-kind-k8s-cluster-with-gpu-support">Create Kind K8s cluster with GPU support<a href="https://www.substratus.ai/blog/kind-local-llama-on-rtx-2060#create-kind-k8s-cluster-with-gpu-support" class="hash-link" aria-label="Direct link to Create Kind K8s cluster with GPU support" title="Direct link to Create Kind K8s cluster with GPU support">​</a></h3>
<p>Install the NVIDIA container toolkit for Docker: <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html" target="_blank" rel="noopener noreferrer">Install Guide</a></p>
<p>Use the convenience script to create a Kind cluster and configure GPU support:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">bash &lt;(curl https://raw.githubusercontent.com/substratusai/substratus/main/install/kind/up-gpu.sh)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Or inspect the <a href="https://github.com/substratusai/substratus/blob/main/install/kind/up-gpu.sh" target="_blank" rel="noopener noreferrer">script</a> and run the steps one by one.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="install-substratus">Install Substratus<a href="https://www.substratus.ai/blog/kind-local-llama-on-rtx-2060#install-substratus" class="hash-link" aria-label="Direct link to Install Substratus" title="Direct link to Install Substratus">​</a></h3>
<p>Install the Substratus K8s operator which will orchestrate model loading and serving:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl apply -f https://raw.githubusercontent.com/substratusai/substratus/main/install/kind/manifests.yaml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="load-the-llama-2-13b-chat-gguf-model">Load the Llama 2 13b chat GGUF model<a href="https://www.substratus.ai/blog/kind-local-llama-on-rtx-2060#load-the-llama-2-13b-chat-gguf-model" class="hash-link" aria-label="Direct link to Load the Llama 2 13b chat GGUF model" title="Direct link to Load the Llama 2 13b chat GGUF model">​</a></h3>
<p>Create a Model resource to load the <a href="https://huggingface.co/substratusai/Llama-2-13B-chat-GGUF" target="_blank" rel="noopener noreferrer">Llama 2 13b chat GGUF model</a></p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token key atrule" style="color:#00a4db">apiVersion</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> substratus.ai/v1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">kind</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> Model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">metadata</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> llama2</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">13b</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">chat</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">gguf</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">spec</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">image</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> substratusai/model</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">loader</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">huggingface</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">params</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> substratusai/Llama</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">2</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">13B</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">chat</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">GGUF</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">files</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">"model.bin"</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl apply -f https://raw.githubusercontent.com/substratusai/substratus/main/examples/llama2-13b-chat-gguf/base-model.yaml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>The model is being downloaded from HuggingFace into your Kind cluster.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="serve-the-model">Serve the model<a href="https://www.substratus.ai/blog/kind-local-llama-on-rtx-2060#serve-the-model" class="hash-link" aria-label="Direct link to Serve the model" title="Direct link to Serve the model">​</a></h3>
<p>Create a Server resource to serve the model:
<a href="https://www.substratus.ai/blog/kind-local-llama-on-rtx-2060#" title="https://raw.githubusercontent.com/substratusai/substratus/main/examples/llama2-13b-chat-gguf/base-model.yaml yaml">embedmd</a>:# (<a href="https://raw.githubusercontent.com/substratusai/substratus/main/examples/llama2-13b-chat-gguf/server-gpu.yaml" target="_blank" rel="noopener noreferrer">https://raw.githubusercontent.com/substratusai/substratus/main/examples/llama2-13b-chat-gguf/server-gpu.yaml</a> yaml)</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token key atrule" style="color:#00a4db">apiVersion</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> substratus.ai/v1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">kind</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> Server</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">metadata</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> llama2</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">13b</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">chat</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">gguf</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">spec</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">image</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> substratusai/model</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">server</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">llama</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">cpp</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain">latest</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">gpu</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">model</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> llama2</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">13b</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">chat</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">gguf</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">params</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">n_gpu_layers</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">30</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">resources</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">gpu</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token key atrule" style="color:#00a4db">count</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl apply -f https://raw.githubusercontent.com/substratusai/substratus/main/examples/llama2-13b-chat-gguf/server-gpu.yaml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Note in my case 30 out of 42 layers loaded into GPU is the max, but you might be able
to load all 42 layers into the GPU if you have more GPU memory.</p>
<p>Once the model is ready it will start serving an OpenAI compatible
API endpoint.</p>
<p>Expose the Server to a local port by using port forwarding:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl port-forward service/llama2-13b-chat-gguf-server 8080:8080</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Let's throw some prompts at it:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">curl http://localhost:8080/v1/completions \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  -H "Content-Type: application/json" \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  -d '{ "prompt": "Who was the first president of the United States?", "stop": ["."]}'</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Checkout the full API docs here: <a href="http://localhost:8080/docs" target="_blank" rel="noopener noreferrer">http://localhost:8080/docs</a></p>
<p>You can play around with other models. For example, if you have a 24 GB GPU card you should
be able to run Llama 2 70B in 4 bit mode by using llama.cpp.</p>
<p>Support the project by adding a star on GitHub! ❤️
<span><a href="https://github.com/substratusai/substratus" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star substratusai/substratus on GitHub">Star</a></span></p>]]></content>
        <author>
            <name>Sam Stoelinga</name>
            <uri>https://github.com/samos123</uri>
        </author>
        <category label="llama" term="llama"/>
        <category label="kind" term="kind"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing: kubectl notebook]]></title>
        <id>https://www.substratus.ai/blog/introducing-kubectl-notebook</id>
        <link href="https://www.substratus.ai/blog/introducing-kubectl-notebook"/>
        <updated>2023-08-22T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Substratus has added the kubectl notebook command!]]></summary>
        <content type="html"><![CDATA[<img src="https://www.substratus.ai/img/kubectl-notebook-cmd.png" alt="kubectl notebook" width="100%">
<!-- -->
<!-- -->
<p><a href="https://github.com/substratusai/substratus" target="_blank" rel="noopener noreferrer">Substratus</a> has added the <code>kubectl notebook</code> command!</p>
<blockquote><p>"Wouldn't it be nice to have a single command that containerized your local directory and served it as a Jupyter Notebook running on a machine with a bunch of GPUs attached?"</p></blockquote>
<p>The conversation went something like that while we daydreamed about our preferred workflow. At that point in time we were hopping back-n-forth between Google Colab and our containers while developing a LLM training job.</p>
<blockquote><p>"Annnddd it should automatically sync file-changes back to your local directory so that you can commit your changes to git and kick off a long-running ML training job - containerized with the exact same python version and packages!"</p></blockquote>
<p>So we built it!</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl notebook -d .</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>And now it has become an integral part of our workflow as we build out the <a href="https://github.com/substratusai/substratus" target="_blank" rel="noopener noreferrer">Substratus ML platform</a>.</p>
<p>Check out the 50 second screenshare:</p>
<div class="video-container"><iframe class="video" src="https://www.youtube-nocookie.com/embed/0_PWl6vjqdE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"></iframe></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="design-goals">Design Goals<a href="https://www.substratus.ai/blog/introducing-kubectl-notebook#design-goals" class="hash-link" aria-label="Direct link to Design Goals" title="Direct link to Design Goals">​</a></h2>
<ol>
<li>One command should build, launch, and sync the Notebook.</li>
<li>Users should only need a Kubeconfig - no other credentials.</li>
<li>Admins should not need to setup networking, TLS, etc.</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="implementation">Implementation<a href="https://www.substratus.ai/blog/introducing-kubectl-notebook#implementation" class="hash-link" aria-label="Direct link to Implementation" title="Direct link to Implementation">​</a></h2>
<p>We tackled our design goals using the following techniques:</p>
<ol>
<li>Implemented as a single Go binary, executed as a <a href="https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/" target="_blank" rel="noopener noreferrer">kubectl plugin</a>.</li>
<li><a href="https://cloud.google.com/storage/docs/access-control/signed-urls" target="_blank" rel="noopener noreferrer">Signed URLs</a> allow for users to upload their local directory to a bucket without requiring cloud credentials (Similar to how popular consumer clouds function).</li>
<li>Kubernetes <a href="https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/" target="_blank" rel="noopener noreferrer">port-forwarding</a> allows for serving remote notebooks without requiring admins to deal with networking / TLS concerns. It also leans on existing Kubernetes RBAC for access control.</li>
</ol>
<p>Some interesting details:</p>
<ul>
<li>Builds are executed remotely for two reasons:<!-- -->
<ul>
<li>Users don't need to install docker.</li>
<li>It avoids pushing massive container images from one's local machine (pip installs often inflate the final docker image to be much larger than the build context itself).</li>
</ul>
</li>
<li>The client requests an upload URL by specifying the MD5 hash it wishes to upload - allowing for server-side signature verification.</li>
<li>Builds are skipped entirely if the MD5 hash of the build context already exists in the bucket.</li>
</ul>
<p>The system underneath the <code>notebook</code> command:</p>
<p><img loading="lazy" alt="diagram" src="https://www.substratus.ai/assets/images/kubectl-notebook.excalidraw-c10c014aec5fc834c0c9f5c2d4ee10d0.png" width="773" height="1536" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="more-to-come">More to come!<a href="https://www.substratus.ai/blog/introducing-kubectl-notebook#more-to-come" class="hash-link" aria-label="Direct link to More to come!" title="Direct link to More to come!">​</a></h2>
<p>Lazy-loading large models from disk...
Incremental dataset loading...
Stay tuned to learn more about how Notebooks on Substratus can speed up your ML workflows.</p>
<p>Don't forget to star and follow the repo!</p>
<p><a href="https://github.com/substratusai/substratus" target="_blank" rel="noopener noreferrer">https://github.com/substratusai/substratus</a></p>
<span><a href="https://github.com/substratusai/substratus" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star substratusai/substratus on GitHub">Star</a></span>]]></content>
        <author>
            <name>Nick Stogner</name>
            <uri>https://github.com/nstogner</uri>
        </author>
        <category label="introduction" term="introduction"/>
        <category label="feature" term="feature"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tutorial: Llama2 70b serving on GKE]]></title>
        <id>https://www.substratus.ai/blog/tutorial-llama2-70b-serving-gke</id>
        <link href="https://www.substratus.ai/blog/tutorial-llama2-70b-serving-gke"/>
        <updated>2023-08-06T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Llama 2 70b is the newest iteration of the Llama model published by Meta, sporting 7 Billion parameters.]]></summary>
        <content type="html"><![CDATA[<p>Llama 2 70b is the newest iteration of the Llama model published by Meta, sporting 7 Billion parameters.
Follow along in this tutorial to get Llama 2 70b deployed on GKE:</p>
<ol>
<li>Create a GKE cluster with Substratus installed.</li>
<li>Load the Llama 2 70b model from HuggingFace.</li>
<li>Serve the model via an interactive inference server.</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="install-substratus-on-gcp">Install Substratus on GCP<a href="https://www.substratus.ai/blog/tutorial-llama2-70b-serving-gke#install-substratus-on-gcp" class="hash-link" aria-label="Direct link to Install Substratus on GCP" title="Direct link to Install Substratus on GCP">​</a></h2>
<p>Use the <a href="https://www.substratus.ai/docs/installation/gcp">Installation Guide for GCP</a> to install Substratus.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="load-the-model-into-substratus">Load the Model into Substratus<a href="https://www.substratus.ai/blog/tutorial-llama2-70b-serving-gke#load-the-model-into-substratus" class="hash-link" aria-label="Direct link to Load the Model into Substratus" title="Direct link to Load the Model into Substratus">​</a></h2>
<p>You will need to agree to HuggingFace's terms before you can use the Llama 2 model. This means you will need to pass your HuggingFace token to Substratus.</p>
<p>Let's tell Substratus how to import Llama 2 by defining a Model resource. Create a file named <code>base-model.yaml</code> with the following content:</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token key atrule" style="color:#00a4db">apiVersion</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> substratus.ai/v1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">kind</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> Model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">metadata</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> llama</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">2</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">70b</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">spec</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">image</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> substratusai/model</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">loader</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">huggingface</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">env</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># You would first have to create a secret named `ai` that</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># has the key `HUGGING_FACE_HUB_TOKEN` set to your token.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># E.g. create the secret by running:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># kubectl create secret generic ai --from-literal="HUGGING_FACE_HUB_TOKEN=&lt;my-token&gt;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">HUGGING_FACE_HUB_TOKEN</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> $</span><span class="token punctuation" style="color:#393A34">{</span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"> secrets.ai.HUGGING_FACE_HUB_TOKEN </span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">params</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> meta</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">llama/Llama</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">2</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">70b</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">hf</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Get your HuggingFace token by going to <a href="https://huggingface.co/settings/tokens" target="_blank" rel="noopener noreferrer">HuggingFace Settings &gt; Access Tokens</a>.</p>
<p>Create a secret with your HuggingFace token:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl create secret generic ai --from-literal="HUGGING_FACE_HUB_TOKEN=&lt;my-token&gt;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Make sure to replace <code>&lt;my-token&gt;</code> with your actual token.</p>
<p>Run the following command to load the base model:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl apply -f base-model.yaml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Watch Substratus kick off your importing Job.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get jobs -w</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>You can view the Job logs by running:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl logs -f jobs/llama-2-70b-modeller</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="serve-the-loaded-model">Serve the Loaded Model<a href="https://www.substratus.ai/blog/tutorial-llama2-70b-serving-gke#serve-the-loaded-model" class="hash-link" aria-label="Direct link to Serve the Loaded Model" title="Direct link to Serve the Loaded Model">​</a></h2>
<p>While the Model is loading, we can define our inference server. Create a file named <code>server.yaml</code> with the following content:</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token key atrule" style="color:#00a4db">apiVersion</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> substratus.ai/v1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">kind</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> Server</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">metadata</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> llama</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">2</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">70b</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">spec</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">image</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> substratusai/model</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">server</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">basaran</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">model</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> llama</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">2</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">70b</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">env</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">MODEL_LOAD_IN_4BIT</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">"true"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">resources</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">gpu</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token key atrule" style="color:#00a4db">type</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> nvidia</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">a100</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token key atrule" style="color:#00a4db">count</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Create the Server by running:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl apply -f server.yaml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Once the Model is loaded (marked as <code>ready</code>), Substratus will automatically launch the server. View the state of both resources using kubectl:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get models,servers</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>To view more information about either the Model or Server, you can use <code>kubectl describe</code>:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl describe -f base-model.yaml</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># OR</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl describe -f server.yaml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Once the model is loaded, the initial server startup time is about 20 minutes.
This is because the model is 100GB+ in size and takes a while to load
into GPU memory.</p>
<p>Look for a log message that the container
is serving at port <code>8080</code>. You can check the logs
by running:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl logs deployment/llama-2-70b-server</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>For demo purposes, you can use port forwarding once the Server is ready on port 8080. Run the following command to forward the container port 8080 to your localhost port 8080:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl port-forward service/llama-2-70b-server 8080:8080</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Interact with Llama 2 in your browser:
<a href="http://localhost:8080/" target="_blank" rel="noopener noreferrer">http://localhost:8080</a></p>
<p><em>You have now deployed Llama 2 70b!</em></p>
<p>You can repeat these steps for other models. For example, you
could instead deploy the "Instruct" variation of Llama.</p>
<p>Stay tuned for another blog post on how to fine-tune Llama 2 70b on your own data.</p>]]></content>
        <author>
            <name>Sam Stoelinga</name>
            <uri>https://github.com/samos123</uri>
        </author>
        <category label="tutorial" term="tutorial"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing Substratus]]></title>
        <id>https://www.substratus.ai/blog/introducing-substratus</id>
        <link href="https://www.substratus.ai/blog/introducing-substratus"/>
        <updated>2023-08-03T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[We are excited to introduce Substratus, the open-source cross-cloud substrate]]></summary>
        <content type="html"><![CDATA[<p>We are excited to introduce Substratus, the open-source cross-cloud substrate
for training and serving ML models with an initial focus on Large Language
Models. Fine-tune and serve LLMs on your Kubernetes clusters in your cloud.</p>
<p>Can’t wait? - Get started with our <a href="https://www.substratus.ai/docs/category/quickstart">quick start
docs</a> or jump over to the <a href="https://github.com/substratusai/substratus" target="_blank" rel="noopener noreferrer">GitHub
repo</a>.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="why-substratus"><strong>Why Substratus?</strong><a href="https://www.substratus.ai/blog/introducing-substratus#why-substratus" class="hash-link" aria-label="Direct link to why-substratus" title="Direct link to why-substratus">​</a></h2>
<p><strong>Press the fast-button for ML</strong>: Leverage out of the box container images to
load a base model, optionally fine-tune with your own dataset and spin up a
model server, all without writing any code.</p>
<p><strong>Notebook-integrated workflows:</strong> Launch a remote, containerized, GPU-enabled
notebook from a local directory with a single command. Develop in the exact same
environment as your long running training jobs.</p>
<p><strong>No vendor lock-in</strong>: Substratus is open-source and can run anywhere Kubernetes
runs.</p>
<p><strong>Keep company data internal</strong>: Deploy in your cloud account. Training data and
inference APIs stay within your company’s network.</p>
<p><strong>Best practices by default:</strong> Substratus models are immutable and contain
information about their lineage. Datasets are imported and snapshotted using
off-the-shelf manifests. Training executes in containerized environments, using
immutable base artifacts. Inference servers are pre-configured to leverage
quantization on supported models. GitOps is built-in, not bolted-on.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="guiding-principles"><strong>Guiding Principles</strong><a href="https://www.substratus.ai/blog/introducing-substratus#guiding-principles" class="hash-link" aria-label="Direct link to guiding-principles" title="Direct link to guiding-principles">​</a></h2>
<p>As we continue to develop Substratus, we’re grounded in the following guiding
principles:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-prioritize-simplicity"><strong>1. Prioritize Simplicity</strong><a href="https://www.substratus.ai/blog/introducing-substratus#1-prioritize-simplicity" class="hash-link" aria-label="Direct link to 1-prioritize-simplicity" title="Direct link to 1-prioritize-simplicity">​</a></h3>
<p>We believe the importance of minimizing complexity in software cannot be
understated. In Substratus, we will work hard to keep complexity to a minimum
as the project grows. The Substratus API currently consists of 4 resource
types: Datasets, Models, Servers, and Notebooks. The project currently depends
on two cloud services outside of the cluster: a bucket and a container registry
(we are working on making these optional too). The project does not (and will
never) depend on a web of complex components like Istio.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-prioritize-ux"><strong>2. Prioritize UX</strong><a href="https://www.substratus.ai/blog/introducing-substratus#2-prioritize-ux" class="hash-link" aria-label="Direct link to 2-prioritize-ux" title="Direct link to 2-prioritize-ux">​</a></h3>
<p>We believe a company’s most precious resource is their engineer’s time.
Substratus seeks to maximize the productivity of data scientists and engineers
through providing a best-in-class user experience. We strive to build a set of
well-designed primitives that allow ML practitioners to enter a flow state as
they move between importing data, training, and serving models.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="roadmap"><strong>Roadmap</strong><a href="https://www.substratus.ai/blog/introducing-substratus#roadmap" class="hash-link" aria-label="Direct link to roadmap" title="Direct link to roadmap">​</a></h2>
<p>We are fast at work adding new functionality, focused on creating the most
productive and enjoyable platform for ML practitioners. Coming soon:</p>
<ol>
<li>Support for AWS and Azure</li>
<li>VS Code Notebook Integration</li>
<li>Large-scale distributed training</li>
<li>ML ecosystem integrations</li>
</ol>
<p>Try Substratus today in your GCP project by following the <a href="https://www.substratus.ai/docs/category/quickstart">quick start
docs</a>. Let us know what features you
would like to see on our <a href="https://github.com/substratusai/substratus" target="_blank" rel="noopener noreferrer">GitHub
repo</a> and don’t forget to add a
star!</p>]]></content>
        <author>
            <name>Brandon Bjelland</name>
            <uri>https://github.com/brandonjbjelland</uri>
        </author>
        <author>
            <name>Nick Stogner</name>
            <uri>https://github.com/nstogner</uri>
        </author>
        <author>
            <name>Sam Stoelinga</name>
            <uri>https://github.com/samos123</uri>
        </author>
        <category label="hello world" term="hello world"/>
        <category label="introduction" term="introduction"/>
        <category label="oss launch" term="oss launch"/>
    </entry>
</feed>