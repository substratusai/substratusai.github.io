<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://www.substratus.ai/blog</id>
    <title>Substratus Blog</title>
    <updated>2023-08-25T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://www.substratus.ai/blog"/>
    <subtitle>Substratus Blog</subtitle>
    <icon>https://www.substratus.ai/img/favicon.ico</icon>
    <entry>
        <title type="html"><![CDATA[A Kind Local Llama on K8s]]></title>
        <id>https://www.substratus.ai/blog/kind-local-llama-on-rtx-2060</id>
        <link href="https://www.substratus.ai/blog/kind-local-llama-on-rtx-2060"/>
        <updated>2023-08-25T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[I've always wanted to have a 🦙 all for myself. So I went ahead and tried]]></summary>
        <content type="html"><![CDATA[<img loading="lazy" src="/img/kind-llama-on-laptop-gpu.png" alt="kubectl notebook" width="100%" class="img_ev3q"><p>I've always wanted to have a 🦙 all for myself. So I went ahead and tried
to deploy the Llama 13B Chat model on a local K8s cluster on my laptop.
The model ran surprisingly well on a mere RTX 2060.</p><p>My laptop setup looks like this:</p><ul><li>Kind for deploying a single node K8s cluster</li><li>AMD Ryzen 7 (8 threads), 16 GB system memory, RTX 2060 (6GB GPU memory)</li><li>Llama.cpp/GGML for fast serving and loading larger models on consumer hardware</li></ul><p>Now you might be thinking how did a 13 billion parameter model fit on just 6GB GPU memory?
Shouldn't that require ~13 GB of GPU memory when serving the model in 4 bit mode?
Yes it should because 13 billion * 4 bytes / (32 bits / 4 bits) = 13 GB.
Luckily, <a href="https://github.com/ggerganov/llama.cpp" target="_blank" rel="noopener noreferrer">Llama.cpp</a> allows us to load models in 2 bit mode and supports running on CPU only at low speeds.</p><p>Want to try this out yourself? Follow a long for a fun ride.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="create-kind-k8s-cluster-with-gpu-support">Create Kind K8s cluster with GPU support<a href="#create-kind-k8s-cluster-with-gpu-support" class="hash-link" aria-label="Direct link to Create Kind K8s cluster with GPU support" title="Direct link to Create Kind K8s cluster with GPU support">​</a></h3><p>Install the NVIDIA container toolkit for Docker: <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html" target="_blank" rel="noopener noreferrer">Install Guide</a></p><p>Use the convenience script to create a Kind cluster and configure GPU support:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token function" style="color:#d73a49">bash</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token punctuation" style="color:#393A34">(</span><span class="token function" style="color:#d73a49">curl</span><span class="token plain"> https://raw.githubusercontent.com/substratusai/substratus/main/install/kind/up-gpu.sh</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Or inspect the <a href="https://github.com/substratusai/substratus/blob/main/install/kind/up-gpu.sh" target="_blank" rel="noopener noreferrer">script</a> and run the steps one by one.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="install-substratus">Install Substratus<a href="#install-substratus" class="hash-link" aria-label="Direct link to Install Substratus" title="Direct link to Install Substratus">​</a></h3><p>Install the Substratus K8s operator which will orchestrate model loading and serving:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl apply -f https://raw.githubusercontent.com/substratusai/substratus/main/install/kind/manifests.yaml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="load-the-llama-2-13b-chat-ggml-model">Load the Llama 2 13b chat GGML model<a href="#load-the-llama-2-13b-chat-ggml-model" class="hash-link" aria-label="Direct link to Load the Llama 2 13b chat GGML model" title="Direct link to Load the Llama 2 13b chat GGML model">​</a></h3><p>Create a Model resource to load the <a href="https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML" target="_blank" rel="noopener noreferrer">Llama 2 13b chat GGML model</a> from The Bloke.</p><div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token key atrule" style="color:#00a4db">apiVersion</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> substratus.ai/v1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">kind</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> Model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">metadata</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> llama2</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">13b</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">chat</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">ggml</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">spec</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">image</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> substratusai/model</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">loader</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">huggingface</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">params</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> TheBloke/Llama</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">2</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">13B</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">chat</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">GGML</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">files</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">"config.json,llama-2-13b-chat.ggmlv3.q2_K.bin"</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl apply -f https://raw.githubusercontent.com/substratusai/substratus/main/examples/llama2-13b-chat-ggml/base-model.yaml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>The model is being downloaded from HuggingFace into your Kind cluster.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="serve-the-model">Serve the model<a href="#serve-the-model" class="hash-link" aria-label="Direct link to Serve the model" title="Direct link to Serve the model">​</a></h3><p>Create a Server resource to serve the model:</p><div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token key atrule" style="color:#00a4db">apiVersion</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> substratus.ai/v1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">kind</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> Server</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">metadata</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> llama2</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">13b</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">chat</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">ggml</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">spec</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">image</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> substratusai/model</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">server</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">llama</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">cpp</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">model</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> llama2</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">13b</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">chat</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">ggml</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">resources</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">gpu</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token key atrule" style="color:#00a4db">count</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl apply -f https://raw.githubusercontent.com/substratusai/substratus/main/examples/llama2-13b-chat-ggml/server.yaml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Once the model is ready it will start serving an OpenAI compatible
API endpoint.</p><p>Expose the Server to a local port by using port forwarding:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl port-forward service/llama2-13b-chat-ggml-server </span><span class="token number" style="color:#36acaa">8080</span><span class="token plain">:8080</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Let's throw some prompts at it:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token function" style="color:#d73a49">curl</span><span class="token plain"> http://localhost:8080/v1/completions </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  -H </span><span class="token string" style="color:#e3116c">"Content-Type: application/json"</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  -d </span><span class="token string" style="color:#e3116c">'{ "prompt": "Who was the first president of the United States?", "stop": ["."]}'</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>You can play around with other models. For example, if you have a 24 GB GPU card you should
be able to run Llama 2 70B in 2 bit mode by using llama.cpp.</p><p>Support the project by adding a star on GitHub! ❤️</p><span><a href="https://github.com/substratusai/substratus" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star substratusai/substratus on GitHub">Star</a></span>]]></content>
        <author>
            <name>Sam Stoelinga</name>
            <uri>https://github.com/samos123</uri>
        </author>
        <category label="llama" term="llama"/>
        <category label="kind" term="kind"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing: kubectl notebook]]></title>
        <id>https://www.substratus.ai/blog/introducing-kubectl-notebook</id>
        <link href="https://www.substratus.ai/blog/introducing-kubectl-notebook"/>
        <updated>2023-08-22T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Substratus has added the kubectl notebook command!]]></summary>
        <content type="html"><![CDATA[<img loading="lazy" src="/img/kubectl-notebook-cmd.png" alt="kubectl notebook" width="100%" class="img_ev3q"><p><a href="https://github.com/substratusai/substratus" target="_blank" rel="noopener noreferrer">Substratus</a> has added the <code>kubectl notebook</code> command!</p><blockquote>"Wouldn't it be nice to have a single command that containerized your local directory and served it as a Jupyter Notebook running on a machine with a bunch of GPUs attached?"</blockquote><p>The conversation went something like that while we daydreamed about our preferred workflow. At that point in time we were hopping back-n-forth between Google Colab and our containers while developing a LLM training job.</p><blockquote>"Annnddd it should automatically sync file-changes back to your local directory so that you can commit your changes to git and kick off a long-running ML training job - containerized with the exact same python version and packages!"</blockquote><p>So we built it!</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl notebook -d </span><span class="token builtin class-name">.</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>And now it has become an integral part of our workflow as we build out the <a href="https://github.com/substratusai/substratus" target="_blank" rel="noopener noreferrer">Substratus ML platform</a>.</p><p>Check out the 50 second screenshare:</p><div class="video-container"><iframe class="video" src="https://www.youtube-nocookie.com/embed/0_PWl6vjqdE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"></iframe></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="design-goals">Design Goals<a href="#design-goals" class="hash-link" aria-label="Direct link to Design Goals" title="Direct link to Design Goals">​</a></h2><ol><li>One command should build, launch, and sync the Notebook.</li><li>Users should only need a Kubeconfig - no other credentials.</li><li>Admins should not need to setup networking, TLS, etc.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="implementation">Implementation<a href="#implementation" class="hash-link" aria-label="Direct link to Implementation" title="Direct link to Implementation">​</a></h2><p>We tackled our design goals using the following techniques:</p><ol><li>Implemented as a single Go binary, executed as a <a href="https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/" target="_blank" rel="noopener noreferrer">kubectl plugin</a>.</li><li><a href="https://cloud.google.com/storage/docs/access-control/signed-urls" target="_blank" rel="noopener noreferrer">Signed URLs</a> allow for users to upload their local directory to a bucket without requiring cloud credentials (Similar to how popular consumer clouds function).</li><li>Kubernetes <a href="https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/" target="_blank" rel="noopener noreferrer">port-forwarding</a> allows for serving remote notebooks without requiring admins to deal with networking / TLS concerns. It also leans on existing Kubernetes RBAC for access control.</li></ol><p>Some interesting details:</p><ul><li>Builds are executed remotely for two reasons:<ul><li>Users don't need to install docker.</li><li>It avoids pushing massive container images from one's local machine (pip installs often inflate the final docker image to be much larger than the build context itself).</li></ul></li><li>The client requests an upload URL by specifying the MD5 hash it wishes to upload - allowing for server-side signature verification.</li><li>Builds are skipped entirely if the MD5 hash of the build context already exists in the bucket.</li></ul><p>The system underneath the <code>notebook</code> command:</p><p><img loading="lazy" alt="diagram" src="/assets/images/kubectl-notebook.excalidraw-c10c014aec5fc834c0c9f5c2d4ee10d0.png" width="773" height="1536" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="more-to-come">More to come!<a href="#more-to-come" class="hash-link" aria-label="Direct link to More to come!" title="Direct link to More to come!">​</a></h2><p>Lazy-loading large models from disk...
Incremental dataset loading...
Stay tuned to learn more about how Notebooks on Substratus can speed up your ML workflows.</p><p>Don't forget to star and follow the repo!</p><p><a href="https://github.com/substratusai/substratus" target="_blank" rel="noopener noreferrer">https://github.com/substratusai/substratus</a></p><span><a href="https://github.com/substratusai/substratus" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star substratusai/substratus on GitHub">Star</a></span>]]></content>
        <author>
            <name>Nick Stogner</name>
            <uri>https://github.com/nstogner</uri>
        </author>
        <category label="introduction" term="introduction"/>
        <category label="feature" term="feature"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tutorial: Llama2 70b serving on GKE]]></title>
        <id>https://www.substratus.ai/blog/tutorial-llama2-70b-serving-gke</id>
        <link href="https://www.substratus.ai/blog/tutorial-llama2-70b-serving-gke"/>
        <updated>2023-08-06T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Llama 2 70b is the newest iteration of the Llama model published by Meta, sporting 7 Billion parameters.]]></summary>
        <content type="html"><![CDATA[<p>Llama 2 70b is the newest iteration of the Llama model published by Meta, sporting 7 Billion parameters.
Follow along in this tutorial to get Llama 2 70b deployed on GKE:</p><ol><li>Create a GKE cluster with Substratus installed.</li><li>Load the Llama 2 70b model from HuggingFace.</li><li>Serve the model via an interactive inference server.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="install-substratus-on-gcp">Install Substratus on GCP<a href="#install-substratus-on-gcp" class="hash-link" aria-label="Direct link to Install Substratus on GCP" title="Direct link to Install Substratus on GCP">​</a></h2><p>Use the <a href="/docs/installation/gcp">Installation Guide for GCP</a> to install Substratus.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="load-the-model-into-substratus">Load the Model into Substratus<a href="#load-the-model-into-substratus" class="hash-link" aria-label="Direct link to Load the Model into Substratus" title="Direct link to Load the Model into Substratus">​</a></h2><p>You will need to agree to HuggingFace's terms before you can use the Llama 2 model. This means you will need to pass your HuggingFace token to Substratus.</p><p>Let's tell Substratus how to import Llama 2 by defining a Model resource. Create a file named <code>base-model.yaml</code> with the following content:</p><div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token key atrule" style="color:#00a4db">apiVersion</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> substratus.ai/v1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">kind</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> Model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">metadata</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> llama</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">2</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">70b</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">spec</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">image</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> substratusai/model</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">loader</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">huggingface</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">params</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> meta</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">llama/Llama</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">2</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">70b</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">hf</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">hugging_face_hub_token</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> $</span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain">HUGGINGFACE_TOKEN</span><span class="token punctuation" style="color:#393A34">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Notice the <code>${HUGGINGFACE_TOKEN}</code> placeholder in the <code>base-model.yaml</code> file.</p><p>Get your HuggingFace token by going to <a href="https://huggingface.co/settings/tokens" target="_blank" rel="noopener noreferrer">HuggingFace Settings &gt; Access Tokens</a>.
Create an environment variable that holds your HuggingFace token:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">export</span><span class="token plain"> </span><span class="token assign-left variable" style="color:#36acaa">HUGGINGFACE_TOKEN</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">replace_me</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Let's use <code>envsubst</code> to set the <code>${HUGGINGFACE_TOKEN}</code> variable when we apply the Model.</p><p>Run the following command:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token function" style="color:#d73a49">cat</span><span class="token plain"> base-model.yaml </span><span class="token operator" style="color:#393A34">|</span><span class="token plain"> envsubst </span><span class="token operator" style="color:#393A34">|</span><span class="token plain"> kubectl apply -f -</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Watch Substratus kick off your importing Job.</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get </span><span class="token function" style="color:#d73a49">jobs</span><span class="token plain"> -w</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>You can view the Job logs by running:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl logs -f jobs/llama-2-70b-modeller</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="serve-the-loaded-model">Serve the Loaded Model<a href="#serve-the-loaded-model" class="hash-link" aria-label="Direct link to Serve the Loaded Model" title="Direct link to Serve the Loaded Model">​</a></h2><p>While the Model is loading, we can define our inference server. Create a file named <code>server.yaml</code> with the following content:</p><div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token key atrule" style="color:#00a4db">apiVersion</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> substratus.ai/v1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">kind</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> Server</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">metadata</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> llama</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">2</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">70b</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">spec</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">image</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> substratusai/model</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">server</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">basaran</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">model</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> llama</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">2</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">70b</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">resources</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">gpu</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token key atrule" style="color:#00a4db">type</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> nvidia</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">a100</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token key atrule" style="color:#00a4db">count</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">2</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Create the Server by running:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl apply -f server.yaml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Once the Model is loaded (marked as <code>ready</code>), Substratus will automatically launch the server. View the state of both resources using kubectl:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get models,servers</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>To view more information about either the Model or Server, you can use <code>kubectl describe</code>:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl describe -f base-model.yaml</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># OR</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl describe -f server.yaml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Once the model is loaded, the initial server startup time is about 20 minutes.
This is because the model is 100GB+ in size and takes a while to load
into GPU memory.</p><p>Look for a log message that the container
is serving at port <code>8080</code>. You can check the logs
by running:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl logs deployment/llama-2-70b-server</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>For demo purposes, you can use port forwarding once the Server is ready on port 8080. Run the following command to forward the container port 8080 to your localhost port 8080:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl port-forward service/llama-2-70b-server </span><span class="token number" style="color:#36acaa">8080</span><span class="token plain">:8080</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Interact with Llama 2 in your browser:
<a href="http://localhost:8080" target="_blank" rel="noopener noreferrer">http://localhost:8080</a></p><p><em>You have now deployed Llama 2 70b!</em></p><p>You can repeat these steps for other models. For example, you
could instead deploy the "Instruct" variation of Llama.</p><p>Stay tuned for another blog post on how to fine-tune Llama 2 70b on your own data.</p>]]></content>
        <author>
            <name>Sam Stoelinga</name>
            <uri>https://github.com/samos123</uri>
        </author>
        <category label="tutorial" term="tutorial"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing Substratus]]></title>
        <id>https://www.substratus.ai/blog/introducing-substratus</id>
        <link href="https://www.substratus.ai/blog/introducing-substratus"/>
        <updated>2023-08-03T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[We are excited to introduce Substratus, the open-source cross-cloud substrate]]></summary>
        <content type="html"><![CDATA[<p>We are excited to introduce Substratus, the open-source cross-cloud substrate
for training and serving ML models with an initial focus on Large Language
Models. Fine-tune and serve LLMs on your Kubernetes clusters in your cloud.</p><p>Can’t wait? - Get started with our <a href="/docs/category/quickstart">quick start
docs</a> or jump over to the <a href="https://github.com/substratusai/substratus" target="_blank" rel="noopener noreferrer">GitHub
repo</a>.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="why-substratus"><strong>Why Substratus?</strong><a href="#why-substratus" class="hash-link" aria-label="Direct link to why-substratus" title="Direct link to why-substratus">​</a></h2><p><strong>Press the fast-button for ML</strong>: Leverage out of the box container images to
load a base model, optionally fine-tune with your own dataset and spin up a
model server, all without writing any code.</p><p><strong>Notebook-integrated workflows:</strong> Launch a remote, containerized, GPU-enabled
notebook from a local directory with a single command. Develop in the exact same
environment as your long running training jobs.</p><p><strong>No vendor lock-in</strong>: Substratus is open-source and can run anywhere Kubernetes
runs.</p><p><strong>Keep company data internal</strong>: Deploy in your cloud account. Training data and
inference APIs stay within your company’s network.</p><p><strong>Best practices by default:</strong> Substratus models are immutable and contain
information about their lineage. Datasets are imported and snapshotted using
off-the-shelf manifests. Training executes in containerized environments, using
immutable base artifacts. Inference servers are pre-configured to leverage
quantization on supported models. GitOps is built-in, not bolted-on.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="guiding-principles"><strong>Guiding Principles</strong><a href="#guiding-principles" class="hash-link" aria-label="Direct link to guiding-principles" title="Direct link to guiding-principles">​</a></h2><p>As we continue to develop Substratus, we’re grounded in the following guiding
principles:</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-prioritize-simplicity"><strong>1. Prioritize Simplicity</strong><a href="#1-prioritize-simplicity" class="hash-link" aria-label="Direct link to 1-prioritize-simplicity" title="Direct link to 1-prioritize-simplicity">​</a></h3><p> We believe the importance of minimizing complexity in software cannot be
understated. In Substratus, we will work hard to keep complexity to a minimum
as the project grows. The Substratus API currently consists of 4 resource
types: Datasets, Models, Servers, and Notebooks. The project currently depends
on two cloud services outside of the cluster: a bucket and a container registry
(we are working on making these optional too). The project does not (and will
never) depend on a web of complex components like Istio.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-prioritize-ux"><strong>2. Prioritize UX</strong><a href="#2-prioritize-ux" class="hash-link" aria-label="Direct link to 2-prioritize-ux" title="Direct link to 2-prioritize-ux">​</a></h3><p>We believe a company’s most precious resource is their engineer’s time.
Substratus seeks to maximize the productivity of data scientists and engineers
through providing a best-in-class user experience. We strive to build a set of
well-designed primitives that allow ML practitioners to enter a flow state as
they move between importing data, training, and serving models.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="roadmap"><strong>Roadmap</strong><a href="#roadmap" class="hash-link" aria-label="Direct link to roadmap" title="Direct link to roadmap">​</a></h2><p>We are fast at work adding new functionality, focused on creating the most
productive and enjoyable platform for ML practitioners. Coming soon:</p><ol><li>Support for AWS and Azure</li><li>VS Code Notebook Integration</li><li>Large-scale distributed training</li><li>ML ecosystem integrations</li></ol><p>Try Substratus today in your GCP project by following the <a href="/docs/category/quickstart">quick start
docs</a>. Let us know what features you
would like to see on our <a href="https://github.com/substratusai/substratus" target="_blank" rel="noopener noreferrer">GitHub
repo</a> and don’t forget to add a
star!</p>]]></content>
        <author>
            <name>Brandon Bjelland</name>
            <uri>https://github.com/brandonjbjelland</uri>
        </author>
        <author>
            <name>Nick Stogner</name>
            <uri>https://github.com/nstogner</uri>
        </author>
        <author>
            <name>Sam Stoelinga</name>
            <uri>https://github.com/samos123</uri>
        </author>
        <category label="hello world" term="hello world"/>
        <category label="introduction" term="introduction"/>
        <category label="oss launch" term="oss launch"/>
    </entry>
</feed>