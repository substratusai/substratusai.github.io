{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 1\n",
    "---\n",
    "\n",
    "# Loading Models\n",
    "\n",
    "<!-- THE MARKDOWN (.md) FILE IS GENERATED FROM THE NOTEBOOK (.ipynb) FILE -->\n",
    "\n",
    "Substratus makes it very easy to import your existing ML models into Substratus.\n",
    "You can for example use the Substratus provided `substratusai/model-loader-huggingface`\n",
    "container image to load models into Substratus.\n",
    "\n",
    "The below Model resource shows an example on how to load the falcon-7b-instruct\n",
    "model into substratus:\n",
    "```yaml\n",
    "apiVersion: substratus.ai/v1\n",
    "kind: Model\n",
    "metadata:\n",
    "  name: falcon-7b-instruct\n",
    "spec:\n",
    "  image:\n",
    "    name: substratusai/model-loader-huggingface\n",
    "  params:\n",
    "    name: tiiuae/falcon-7b-instruct\n",
    "```\n",
    "\n",
    "In the model spec, you provide the container image location and the name\n",
    "of the HuggingFace model. The model-loader-huggingface downloades the model\n",
    "and stores it in a predefined location such that the model gets stored\n",
    "in a cloud storage bucket (e.g. a GCS bucket).\n",
    "\n",
    "The Model.status.url will provide you with the cloud storage location\n",
    "where the model is stored.\n",
    "\n",
    "\n",
    "## Loading a Model from HuggingFace\n",
    "Run the following command to load the falcon-7b-instruct model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl apply -f https://raw.githubusercontent.com/substratusai/substratus/main/examples/falcon-7b-instruct/base-model.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you create the model, it will create a K8s job to load the HuggingFace model\n",
    "into Substratus. This job will use the container image defined in the Model resource.\n",
    "\n",
    "You can take a look at the logs of the job by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl logs jobs/falcon-7b-instruct-modeller | tail -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After about 5 minutes the job should finish and the Model resource should report the status\n",
    "to be ready. Verify by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:         falcon-7b-instruct\n",
      "Namespace:    default\n",
      "Labels:       <none>\n",
      "Annotations:  <none>\n",
      "API Version:  substratus.ai/v1\n",
      "Kind:         Model\n",
      "Metadata:\n",
      "  Creation Timestamp:  2023-07-15T02:46:05Z\n",
      "  Generation:          1\n",
      "  Resource Version:    14266797\n",
      "  UID:                 077198a0-32ec-4f07-9bc3-ba3a1f1a3729\n",
      "Spec:\n",
      "  Image:\n",
      "    Name:  substratusai/model-loader-huggingface\n",
      "  Params:\n",
      "    Name:  tiiuae/falcon-7b-instruct\n",
      "Status:\n",
      "  Conditions:\n",
      "    Last Transition Time:  2023-07-15T02:51:18Z\n",
      "    Message:               \n",
      "    Observed Generation:   1\n",
      "    Reason:                JobComplete\n",
      "    Status:                True\n",
      "    Type:                  Modelled\n",
      "  Ready:                   true\n",
      "  URL:                     gs://my-gcs-bucket-name/077198a0-32ec-4f07-9bc3-ba3a1f1a3729/\n",
      "Events:                    <none>\n"
     ]
    }
   ],
   "source": [
    "! kubectl describe model falcon-7b-instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the Status.URL field repots the location of where the model is saved.\n",
    "\n",
    "You have now succesfully loaded a HuggingFace model in Substratus and can use that\n",
    "model in other Substratus resources such as the Server resource.\n",
    "\n",
    "\n",
    "## Loading models from other sources\n",
    "If needed you can provide your own container image as long as it meets\n",
    "the container contract defined by Substratus. For\n",
    "now please file an [Issue on GitHub](\n",
    "  https://github.com/substratusai/substratus/issues\n",
    ") to request other images.\n",
    "\n",
    "## Next steps\n",
    "- Follow the [Serving Models walkthrough](./serving-models.md) to expose the falcon-7b-instruct model\n",
    "and sent it some prompts.\n",
    "- Follow the [Loading Datasets walktrhough](./loading-datasets.md) so you can finetune\n",
    "  model that you loaded with your own dataset\n",
    "- Follow the [Finetuing walkthrough](./finetuning-models.md) to finetune a model that you loaded from HuggingFace\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
