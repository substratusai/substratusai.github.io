"use strict";(self.webpackChunksubstratus_website=self.webpackChunksubstratus_website||[]).push([[1714],{3905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>g});var l=a(7294);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);t&&(l=l.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,l)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,l,n=function(e,t){if(null==e)return{};var a,l,n={},r=Object.keys(e);for(l=0;l<r.length;l++)a=r[l],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(l=0;l<r.length;l++)a=r[l],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var i=l.createContext({}),u=function(e){var t=l.useContext(i),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},p=function(e){var t=u(e.components);return l.createElement(i.Provider,{value:t},e.children)},m="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return l.createElement(l.Fragment,{},t)}},d=l.forwardRef((function(e,t){var a=e.components,n=e.mdxType,r=e.originalType,i=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),m=u(a),d=n,g=m["".concat(i,".").concat(d)]||m[d]||c[d]||r;return a?l.createElement(g,o(o({ref:t},p),{},{components:a})):l.createElement(g,o({ref:t},p))}));function g(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var r=a.length,o=new Array(r);o[0]=d;var s={};for(var i in t)hasOwnProperty.call(t,i)&&(s[i]=t[i]);s.originalType=e,s[m]="string"==typeof e?e:n,o[1]=s;for(var u=2;u<r;u++)o[u]=a[u];return l.createElement.apply(null,o)}return l.createElement.apply(null,a)}d.displayName="MDXCreateElement"},8753:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>u,contentTitle:()=>s,default:()=>d,frontMatter:()=>o,metadata:()=>i,toc:()=>p});var l=a(7462),n=(a(7294),a(3905)),r=a(2719);const o={slug:"kind-local-llama-on-rtx-2060",title:"A Kind Local Llama on K8s",authors:[{name:"Sam Stoelinga",title:"Engineer",url:"https://github.com/samos123"}],image:"/img/kind-llama-on-laptop-gpu.png",tags:["llama","kind"]},s=void 0,i={permalink:"/blog/kind-local-llama-on-rtx-2060",editUrl:"https://github.com/substratusai/substratusai.github.io/tree/main/blog/2023-08-25-a-kind-llama.md",source:"@site/blog/2023-08-25-a-kind-llama.md",title:"A Kind Local Llama on K8s",description:"I've always wanted to have a \ud83e\udd99 all for myself. So I went ahead and tried",date:"2023-08-25T00:00:00.000Z",formattedDate:"August 25, 2023",tags:[{label:"llama",permalink:"/blog/tags/llama"},{label:"kind",permalink:"/blog/tags/kind"}],readingTime:2.295,hasTruncateMarker:!1,authors:[{name:"Sam Stoelinga",title:"Engineer",url:"https://github.com/samos123"}],frontMatter:{slug:"kind-local-llama-on-rtx-2060",title:"A Kind Local Llama on K8s",authors:[{name:"Sam Stoelinga",title:"Engineer",url:"https://github.com/samos123"}],image:"/img/kind-llama-on-laptop-gpu.png",tags:["llama","kind"]},nextItem:{title:"Introducing: kubectl notebook",permalink:"/blog/introducing-kubectl-notebook"}},u={authorsImageUrls:[void 0]},p=[{value:"Create Kind K8s cluster with GPU support",id:"create-kind-k8s-cluster-with-gpu-support",level:3},{value:"Install Substratus",id:"install-substratus",level:3},{value:"Load the Llama 2 13b chat GGML model",id:"load-the-llama-2-13b-chat-ggml-model",level:3},{value:"Serve the model",id:"serve-the-model",level:3}],m={toc:p},c="wrapper";function d(e){let{components:t,...a}=e;return(0,n.kt)(c,(0,l.Z)({},m,a,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("img",{src:"/img/kind-llama-on-laptop-gpu.png",alt:"kubectl notebook",width:"100%"}),(0,n.kt)("p",null,"I've always wanted to have a \ud83e\udd99 all for myself. So I went ahead and tried\nto deploy the Llama 13B Chat model on a local K8s cluster on my laptop.\nThe model ran surprisingly well on a mere RTX 2060."),(0,n.kt)("p",null,"My laptop setup looks like this:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Kind for deploying a single node K8s cluster"),(0,n.kt)("li",{parentName:"ul"},"AMD Ryzen 7 (8 threads), 16 GB system memory, RTX 2060 (6GB GPU memory)"),(0,n.kt)("li",{parentName:"ul"},"Llama.cpp/GGML for fast serving and loading larger models on consumer hardware")),(0,n.kt)("p",null,"Now you might be thinking how did a 13 billion parameter model fit on just 6GB GPU memory?\nShouldn't that require ~13 GB of GPU memory when serving the model in 4 bit mode?\nYes it should because 13 billion * 4 bytes / (32 bits / 4 bits) = 13 GB.\nLuckily, ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/ggerganov/llama.cpp"},"Llama.cpp")," allows us to load models in 2 bit mode and supports running on CPU only at low speeds."),(0,n.kt)("p",null,"Want to try this out yourself? Follow a long for a fun ride."),(0,n.kt)("h3",{id:"create-kind-k8s-cluster-with-gpu-support"},"Create Kind K8s cluster with GPU support"),(0,n.kt)("p",null,"Install the NVIDIA container toolkit for Docker: ",(0,n.kt)("a",{parentName:"p",href:"https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html"},"Install Guide")),(0,n.kt)("p",null,"Use the convenience script to create a Kind cluster and configure GPU support:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"bash <(curl https://raw.githubusercontent.com/substratusai/substratus/main/install/kind/up-gpu.sh)\n")),(0,n.kt)("p",null,"Or inspect the ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/substratusai/substratus/blob/main/install/kind/up-gpu.sh"},"script")," and run the steps one by one."),(0,n.kt)("h3",{id:"install-substratus"},"Install Substratus"),(0,n.kt)("p",null,"Install the Substratus K8s operator which will orchestrate model loading and serving:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl apply -f https://raw.githubusercontent.com/substratusai/substratus/main/install/kind/manifests.yaml\n")),(0,n.kt)("h3",{id:"load-the-llama-2-13b-chat-ggml-model"},"Load the Llama 2 13b chat GGML model"),(0,n.kt)("p",null,"Create a Model resource to load the ",(0,n.kt)("a",{parentName:"p",href:"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML"},"Llama 2 13b chat GGML model")," from The Bloke."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: substratus.ai/v1\nkind: Model\nmetadata:\n  name: llama2-13b-chat-ggml\nspec:\n  image: substratusai/model-loader-huggingface\n  params:\n    name: TheBloke/Llama-2-13B-chat-GGML\n    files: "config.json,llama-2-13b-chat.ggmlv3.q2_K.bin"\n')),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl apply -f https://raw.githubusercontent.com/substratusai/substratus/main/examples/llama2-13b-chat-ggml/base-model.yaml\n")),(0,n.kt)("p",null,"The model is being downloaded from HuggingFace into your Kind cluster."),(0,n.kt)("h3",{id:"serve-the-model"},"Serve the model"),(0,n.kt)("p",null,"Create a Server resource to serve the model:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: substratus.ai/v1\nkind: Server\nmetadata:\n  name: llama2-13b-chat-ggml\nspec:\n  image: substratusai/model-server-llama-cpp\n  model:\n    name: llama2-13b-chat-ggml\n  resources:\n    gpu:\n      count: 1\n")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl apply -f https://raw.githubusercontent.com/substratusai/substratus/main/examples/llama2-13b-chat-ggml/server.yaml\n")),(0,n.kt)("p",null,"Once the model is ready it will start serving an OpenAI compatible\nAPI endpoint."),(0,n.kt)("p",null,"Expose the Server to a local port by using port forwarding:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl port-forward service/llama2-13b-chat-ggml-server 8080:8080\n")),(0,n.kt)("p",null,"Let's throw some prompts at it:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},'curl http://localhost:8080/v1/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{ "prompt": "Who was the first president of the United States?", "stop": ["."]}\'\n')),(0,n.kt)("p",null,"You can play around with other models. For example, if you have a 24 GB GPU card you should\nbe able to run Llama 2 70B in 2 bit mode by using llama.cpp."),(0,n.kt)("p",null,"Support the project by adding a star on GitHub! \u2764\ufe0f"),(0,n.kt)(r.Z,{href:"https://github.com/substratusai/substratus","data-icon":"octicon-star","data-size":"large","data-show-count":"true","aria-label":"Star substratusai/substratus on GitHub",mdxType:"GitHubButton"},"Star"))}d.isMDXComponent=!0}}]);