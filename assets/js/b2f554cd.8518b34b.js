"use strict";(self.webpackChunksubstratus_website=self.webpackChunksubstratus_website||[]).push([[477],{10:t=>{t.exports=JSON.parse('{"blogPosts":[{"id":"llama2-70b","metadata":{"permalink":"/blog/llama2-70b","editUrl":"https://github.com/substratusai/substratusai.github.io/tree/main/blog/2023-08-06-llama2-70b.md","source":"@site/blog/2023-08-06-llama2-70b.md","title":"Tutorial: Llama2 70b on GKE","description":"Llama 2 70b, is the newest iteration of the Llama model published by Meta.","date":"2023-08-06T00:00:00.000Z","formattedDate":"August 6, 2023","tags":[{"label":"tutorial","permalink":"/blog/tags/tutorial"}],"readingTime":2.555,"hasTruncateMarker":false,"authors":[{"name":"Sam Stoelinga","title":"Engineer","url":"https://github.com/samos123","image_url":"https://avatars.githubusercontent.com/u/388784?v=4","imageURL":"https://avatars.githubusercontent.com/u/388784?v=4"}],"frontMatter":{"slug":"llama2-70b","title":"Tutorial: Llama2 70b on GKE","authors":[{"name":"Sam Stoelinga","title":"Engineer","url":"https://github.com/samos123","image_url":"https://avatars.githubusercontent.com/u/388784?v=4","imageURL":"https://avatars.githubusercontent.com/u/388784?v=4"}],"tags":["tutorial"]},"nextItem":{"title":"Introducing Substratus","permalink":"/blog/introducing-substratus"}},"content":"Llama 2 70b, is the newest iteration of the Llama model published by Meta.\\nFollow along in this tutorial to get Llama 2 70b deployed on GKE:\\n\\n1. Install Substratus on GKE\\n2. Load the Llama 2 70b model\\n3. Serve Llama 2 70b \\n\\n## Install Substratus on GCP\\nUse the Substratus installer to create a new GKE cluster, GCS bucket\\nand Artifact Registry:\\n\\n```bash\\ndocker run -it \\\\\\n  -v ${HOME}/.kube:/root/.kube \\\\\\n  -e PROJECT=$(gcloud config get project) \\\\\\n  -e TOKEN=$(gcloud auth print-access-token) \\\\\\n  substratusai/installer:latest gcp-up.sh\\n```\\n\\n## Load the Llama 2 70b model\\nYou need to agree to HuggingFace\'s terms before you can use the Llama 2 model. This means you can\'t simply download the model without logging into HuggingFace. But don\'t worry, the Substratus model-loader-huggingface can use your HuggingFace token.\\n\\nLet\'s get started with creating the Substrats `Model` resource. Create a file named `base-model.yaml` with the following content:\\n```yaml\\napiVersion: substratus.ai/v1\\nkind: Model\\nmetadata:\\n  name: llama-2-70b\\nspec:\\n  image: substratusai/model-loader-huggingface\\n  params:\\n    name: meta-llama/Llama-2-70b-hf\\n    hugging_face_hub_token: ${HUGGINGFACE_TOKEN}\\n```\\nNotice this part `${HUGGINGFACE_TOKEN}` in the `base-model.yaml` file.\\n\\nGet your HuggingFace token by going to [HuggingFace Settings > Access Tokens](\\n    https://huggingface.co/settings/tokens\\n).\\nCreate an environment variable that holds your HuggingFace token:\\n```bash\\nexport HUGGINGFACE_TOKEN=replace_me\\n```\\n\\nLet\'s use `envsubst` to substitute the `${HUGGINGFACE_TOKEN}` part in in the `base-model.yaml` file with your valid HuggingFace token that you set as environment variable in the previous step.\\n\\nRun the following command:\\n```bash\\ncat base-model.yaml | envsubst | kubectl apply -f -\\n```\\n\\nYou can watch the progress by running:\\n```bash\\nkubectl logs -f jobs/llama-2-70b-modeller\\n```\\n\\nWait till the model reports being ready:\\n```bash\\nkubectl describe model llama-2-70b\\n```\\n\\n## Serve the loaded Llama 2 70b model\\nOnce the model is loaded you can create a Substratus Server\\nresource to serve the model.\\n\\nCreate a file named `server.yaml` with the following content:\\n```yaml\\napiVersion: substratus.ai/v1\\nkind: Server\\nmetadata:\\n  name: llama-2-70b\\nspec:\\n  image: substratusai/model-server-basaran\\n  model:\\n    name: llama-2-70b\\n  resources:\\n    gpu:\\n      type: nvidia-a100\\n      count: 2\\n```\\n\\nCreate the server by running:\\n```bash\\nkubectl apply -f server.yaml\\n```\\n\\nThe initial startup time is about 20 minutes.\\nThis is because the model is 100GB+ in size and it\\nneeds to load the data from GCS into GPU memory.\\n\\nWait until you see a log message that the container\\nis serving at port `8080`. You can check the logs\\nby running:\\n```bash\\nkubectl logs deployment/llama-2-70b-server\\n```\\n\\nYou can then use port forwarding once the Server is ready on port 8080. Run the following command to forward the container port 8080 to your localhost port 8080:\\n```bash\\nkubectl port-forward service/llama-2-70b-server 8080:8080\\n```\\n\\nIn your browser you can now open the following URL:\\n[http://localhost:8080](http://localhost:8080)\\n\\nYou have now deployed the Llama 2 70b base model. You\\ncan repeat the steps for other models, for example, you\\ncould instead deploy llama-2-70b-instruct-v2.\\n\\nStay tuned for another blog post on how to fine tune Llama 2 70B.\\n\\n## Cleanup\\nRun the following command to delete all the Substratus created resources:\\n```bash\\ndocker run -it \\\\\\n  -v ${HOME}/.kube:/root/.kube \\\\\\n  -e PROJECT=$(gcloud config get project) \\\\\\n  -e TOKEN=$(gcloud auth print-access-token) \\\\\\n  substratusai/installer:latest gcp-down.sh\\n```"},{"id":"introducing-substratus","metadata":{"permalink":"/blog/introducing-substratus","editUrl":"https://github.com/substratusai/substratusai.github.io/tree/main/blog/2023-08-03-introducing-substratus.md","source":"@site/blog/2023-08-03-introducing-substratus.md","title":"Introducing Substratus","description":"We are excited to introduce Substratus, the open-source cross-cloud substrate","date":"2023-08-03T00:00:00.000Z","formattedDate":"August 3, 2023","tags":[{"label":"hello world","permalink":"/blog/tags/hello-world"},{"label":"introduction","permalink":"/blog/tags/introduction"},{"label":"oss launch","permalink":"/blog/tags/oss-launch"}],"readingTime":2.175,"hasTruncateMarker":false,"authors":[{"name":"Brandon Bjelland","title":"Co-founding Engineer","url":"https://github.com/brandonjbjelland","image_url":"https://avatars.githubusercontent.com/u/2502520?v=4","imageURL":"https://avatars.githubusercontent.com/u/2502520?v=4"},{"name":"Nick Stogner","title":"Co-founding Engineer","url":"https://github.com/nstogner","image_url":"https://avatars.githubusercontent.com/u/10274189?v=4","imageURL":"https://avatars.githubusercontent.com/u/10274189?v=4"},{"name":"Sam Stoelinga","title":"Co-founding Engineer","url":"https://github.com/samos123","image_url":"https://avatars.githubusercontent.com/u/388784?v=4","imageURL":"https://avatars.githubusercontent.com/u/388784?v=4"}],"frontMatter":{"slug":"introducing-substratus","title":"Introducing Substratus","authors":[{"name":"Brandon Bjelland","title":"Co-founding Engineer","url":"https://github.com/brandonjbjelland","image_url":"https://avatars.githubusercontent.com/u/2502520?v=4","imageURL":"https://avatars.githubusercontent.com/u/2502520?v=4"},{"name":"Nick Stogner","title":"Co-founding Engineer","url":"https://github.com/nstogner","image_url":"https://avatars.githubusercontent.com/u/10274189?v=4","imageURL":"https://avatars.githubusercontent.com/u/10274189?v=4"},{"name":"Sam Stoelinga","title":"Co-founding Engineer","url":"https://github.com/samos123","image_url":"https://avatars.githubusercontent.com/u/388784?v=4","imageURL":"https://avatars.githubusercontent.com/u/388784?v=4"}],"tags":["hello world","introduction","oss launch"]},"prevItem":{"title":"Tutorial: Llama2 70b on GKE","permalink":"/blog/llama2-70b"}},"content":"We are excited to introduce Substratus, the open-source cross-cloud substrate\\nfor training and serving ML models with an initial focus on Large Language\\nModels. Fine-tune and serve LLMs on your Kubernetes clusters in your cloud.\\n\\nCan\u2019t wait? - Get started with our [quick start\\ndocs](https://www.substratus.ai/docs/quickstart) or hop over to our [GitHub\\nrepo](https://github.com/substratusai/substratus).\\n\\n## **Why Substratus?**\\n\\n**Press the fast-button for ML**: Leverage out of the box container images to\\nload a base model, optionally fine-tune with your own dataset and spin up a\\nmodel server, all without writing any code.\\n\\n**Notebook-integrated workflows:** Launch a remote, containerized, GPU-enabled\\nnotebook from a local directory with a single command. Develop in the exact same\\nenvironment as your long running training jobs.\\n\\n**No vendor lock-in**: Substratus is open-source and can run anywhere Kubernetes\\nruns.\\n\\n**Keep company data internal**: Deploy in your cloud account. Training data and\\ninference APIs stay within your company\u2019s network.\\n\\n**Best practices by default:** Substratus models are immutable and contain\\ninformation about their lineage. Datasets are imported and snapshotted using\\noff-the-shelf manifests. Training executes in containerized environments, using\\nimmutable base artifacts. Inference servers are pre-configured to leverage\\nquantization on supported models. GitOps is built-in, not bolted-on.\\n\\n## **Guiding Principles**\\n\\nAs we continue to develop Substratus, we\u2019re grounded in the following guiding\\nprinciples:\\n\\n### **1. Prioritize Simplicity**\\n\\n We believe the importance of minimizing complexity in software cannot be\\n understated. In Substratus, we will work hard to keep complexity to a minimum\\n as the project grows. The Substratus API currently consists of 4 resource\\n types: Datasets, Models, Servers, and Notebooks. The project currently depends\\n on two cloud services outside of the cluster: a bucket and a container registry\\n (we are working on making these optional too). The project does not (and will\\n never) depend on a web of complex components like Istio.\\n\\n### **2. Prioritize UX**\\n\\nWe believe a company\u2019s most precious resource is their engineer\u2019s time.\\nSubstratus seeks to maximize the productivity of data scientists and engineers\\nthrough providing a best-in-class user experience. We strive to build a set of\\nwell-designed primitives that allow ML practitioners to enter a flow state as\\nthey move between importing data, training, and serving models.\\n\\n## **Roadmap**\\n\\nWe are fast at work adding new functionality, focused on creating the most\\nproductive and enjoyable platform for ML practitioners. Coming soon:\\n\\n1. Support for AWS and Azure\\n2. VS Code Notebook Integration\\n3. Large-scale distributed training\\n4. ML ecosystem integrations\\n\\nTry Substratus today in your GCP project by following the [quick start\\ndocs](https://www.substratus.ai/docs/quickstart). Let us know what features you\\nwould like to see on our [GitHub\\nrepo](https://github.com/substratusai/substratus) and don\u2019t forget to add a\\nstar!"}]}')}}]);