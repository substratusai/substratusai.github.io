"use strict";(self.webpackChunksubstratus_website=self.webpackChunksubstratus_website||[]).push([[477],{10:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"tutorial-llama2-70b-serving-gke","metadata":{"permalink":"/blog/tutorial-llama2-70b-serving-gke","editUrl":"https://github.com/substratusai/substratusai.github.io/tree/main/blog/2023-08-06-llama2-70b.md","source":"@site/blog/2023-08-06-llama2-70b.md","title":"Tutorial: Llama2 70b serving on GKE","description":"Llama 2 70b is the newest iteration of the Llama model published by Meta, sporting 7 Billion parameters.","date":"2023-08-06T00:00:00.000Z","formattedDate":"August 6, 2023","tags":[{"label":"tutorial","permalink":"/blog/tags/tutorial"}],"readingTime":2.77,"hasTruncateMarker":false,"authors":[{"name":"Sam Stoelinga","title":"Engineer","url":"https://github.com/samos123","image_url":"https://avatars.githubusercontent.com/u/388784?v=4","imageURL":"https://avatars.githubusercontent.com/u/388784?v=4"}],"frontMatter":{"slug":"tutorial-llama2-70b-serving-gke","title":"Tutorial: Llama2 70b serving on GKE","authors":[{"name":"Sam Stoelinga","title":"Engineer","url":"https://github.com/samos123","image_url":"https://avatars.githubusercontent.com/u/388784?v=4","imageURL":"https://avatars.githubusercontent.com/u/388784?v=4"}],"tags":["tutorial"]},"nextItem":{"title":"Introducing Substratus","permalink":"/blog/introducing-substratus"}},"content":"Llama 2 70b is the newest iteration of the Llama model published by Meta, sporting 7 Billion parameters.\\nFollow along in this tutorial to get Llama 2 70b deployed on GKE:\\n\\n1. Create a GKE cluster with Substratus installed.\\n2. Load the Llama 2 70b model from HuggingFace.\\n3. Serve the model via an interactive inference server.\\n\\n## Install Substratus on GCP\\nUse the Substratus installer to create a new GKE cluster with supporting infrastructure (a GCS bucket\\nand Artifact Registry):\\n\\n```bash\\ndocker run -it \\\\\\n  -v ${HOME}/.kube:/root/.kube \\\\\\n  -e PROJECT=$(gcloud config get project) \\\\\\n  -e TOKEN=$(gcloud auth print-access-token) \\\\\\n  substratusai/installer:v0.8.0 gcp-up.sh\\n```\\n\\n## Load the Model into Substratus\\nYou will need to agree to HuggingFace\'s terms before you can use the Llama 2 model. This means you will need to pass your HuggingFace token to Substratus.\\n\\nLet\'s tell Substratus how to import Llama 2 by defining a Model resource. Create a file named `base-model.yaml` with the following content:\\n```yaml\\napiVersion: substratus.ai/v1\\nkind: Model\\nmetadata:\\n  name: llama-2-70b\\nspec:\\n  image: substratusai/model-loader-huggingface\\n  params:\\n    name: meta-llama/Llama-2-70b-hf\\n    hugging_face_hub_token: ${HUGGINGFACE_TOKEN}\\n```\\nNotice the `${HUGGINGFACE_TOKEN}` placeholder in the `base-model.yaml` file.\\n\\nGet your HuggingFace token by going to [HuggingFace Settings > Access Tokens](\\n    https://huggingface.co/settings/tokens\\n).\\nCreate an environment variable that holds your HuggingFace token:\\n```bash\\nexport HUGGINGFACE_TOKEN=replace_me\\n```\\n\\nLet\'s use `envsubst` to set the `${HUGGINGFACE_TOKEN}` variable when we apply the Model.\\n\\nRun the following command:\\n```bash\\ncat base-model.yaml | envsubst | kubectl apply -f -\\n```\\n\\nWatch Substratus kick off your importing Job.\\n\\n```bash\\nkubectl get jobs -w\\n```\\n\\nYou can view the Job logs by running:\\n```bash\\nkubectl logs -f jobs/llama-2-70b-modeller\\n```\\n\\n## Serve the Loaded Model\\nWhile the Model is loading, we can define our inference server. Create a file named `server.yaml` with the following content:\\n```yaml\\napiVersion: substratus.ai/v1\\nkind: Server\\nmetadata:\\n  name: llama-2-70b\\nspec:\\n  image: substratusai/model-server-basaran\\n  model:\\n    name: llama-2-70b\\n  resources:\\n    gpu:\\n      type: nvidia-a100\\n      count: 2\\n```\\n\\nCreate the Server by running:\\n```bash\\nkubectl apply -f server.yaml\\n```\\n\\nOnce the Model is loaded (marked as `ready`), Substratus will automatically launch the server. View the state of both resources using kubectl:\\n\\n```bash\\nkubectl get models,servers\\n```\\n\\nTo view more information about either the Model or Server, you can use `kubectl describe`:\\n\\n```bash\\nkubectl describe -f base-model.yaml\\n# OR\\nkubectl describe -f server.yaml\\n```\\n\\nOnce the model is loaded, the initial server startup time is about 20 minutes.\\nThis is because the model is 100GB+ in size and takes a while to load\\ninto GPU memory.\\n\\nLook for a log message that the container\\nis serving at port `8080`. You can check the logs\\nby running:\\n```bash\\nkubectl logs deployment/llama-2-70b-server\\n```\\n\\nFor demo purposes, you can use port forwarding once the Server is ready on port 8080. Run the following command to forward the container port 8080 to your localhost port 8080:\\n```bash\\nkubectl port-forward service/llama-2-70b-server 8080:8080\\n```\\n\\nInteract with Llama 2 in your browser:\\n[http://localhost:8080](http://localhost:8080)\\n\\n*You have now deployed Llama 2 70b!*\\n\\nYou can repeat these steps for other models. For example, you\\ncould instead deploy the \\"Instruct\\" variation of Llama.\\n\\nStay tuned for another blog post on how to fine-tune Llama 2 70b on your own data.\\n\\n## Cleanup\\nRun the following command to delete all resources:\\n```bash\\ndocker run -it \\\\\\n  -v ${HOME}/.kube:/root/.kube \\\\\\n  -e PROJECT=$(gcloud config get project) \\\\\\n  -e TOKEN=$(gcloud auth print-access-token) \\\\\\n  substratusai/installer:v0.8.0 gcp-down.sh\\n```"},{"id":"introducing-substratus","metadata":{"permalink":"/blog/introducing-substratus","editUrl":"https://github.com/substratusai/substratusai.github.io/tree/main/blog/2023-08-03-introducing-substratus.md","source":"@site/blog/2023-08-03-introducing-substratus.md","title":"Introducing Substratus","description":"We are excited to introduce Substratus, the open-source cross-cloud substrate","date":"2023-08-03T00:00:00.000Z","formattedDate":"August 3, 2023","tags":[{"label":"hello world","permalink":"/blog/tags/hello-world"},{"label":"introduction","permalink":"/blog/tags/introduction"},{"label":"oss launch","permalink":"/blog/tags/oss-launch"}],"readingTime":2.175,"hasTruncateMarker":false,"authors":[{"name":"Brandon Bjelland","title":"Co-founding Engineer","url":"https://github.com/brandonjbjelland","image_url":"https://avatars.githubusercontent.com/u/2502520?v=4","imageURL":"https://avatars.githubusercontent.com/u/2502520?v=4"},{"name":"Nick Stogner","title":"Co-founding Engineer","url":"https://github.com/nstogner","image_url":"https://avatars.githubusercontent.com/u/10274189?v=4","imageURL":"https://avatars.githubusercontent.com/u/10274189?v=4"},{"name":"Sam Stoelinga","title":"Co-founding Engineer","url":"https://github.com/samos123","image_url":"https://avatars.githubusercontent.com/u/388784?v=4","imageURL":"https://avatars.githubusercontent.com/u/388784?v=4"}],"frontMatter":{"slug":"introducing-substratus","title":"Introducing Substratus","authors":[{"name":"Brandon Bjelland","title":"Co-founding Engineer","url":"https://github.com/brandonjbjelland","image_url":"https://avatars.githubusercontent.com/u/2502520?v=4","imageURL":"https://avatars.githubusercontent.com/u/2502520?v=4"},{"name":"Nick Stogner","title":"Co-founding Engineer","url":"https://github.com/nstogner","image_url":"https://avatars.githubusercontent.com/u/10274189?v=4","imageURL":"https://avatars.githubusercontent.com/u/10274189?v=4"},{"name":"Sam Stoelinga","title":"Co-founding Engineer","url":"https://github.com/samos123","image_url":"https://avatars.githubusercontent.com/u/388784?v=4","imageURL":"https://avatars.githubusercontent.com/u/388784?v=4"}],"tags":["hello world","introduction","oss launch"]},"prevItem":{"title":"Tutorial: Llama2 70b serving on GKE","permalink":"/blog/tutorial-llama2-70b-serving-gke"}},"content":"We are excited to introduce Substratus, the open-source cross-cloud substrate\\nfor training and serving ML models with an initial focus on Large Language\\nModels. Fine-tune and serve LLMs on your Kubernetes clusters in your cloud.\\n\\nCan\u2019t wait? - Get started with our [quick start\\ndocs](https://www.substratus.ai/docs/quickstart) or hop over to our [GitHub\\nrepo](https://github.com/substratusai/substratus).\\n\\n## **Why Substratus?**\\n\\n**Press the fast-button for ML**: Leverage out of the box container images to\\nload a base model, optionally fine-tune with your own dataset and spin up a\\nmodel server, all without writing any code.\\n\\n**Notebook-integrated workflows:** Launch a remote, containerized, GPU-enabled\\nnotebook from a local directory with a single command. Develop in the exact same\\nenvironment as your long running training jobs.\\n\\n**No vendor lock-in**: Substratus is open-source and can run anywhere Kubernetes\\nruns.\\n\\n**Keep company data internal**: Deploy in your cloud account. Training data and\\ninference APIs stay within your company\u2019s network.\\n\\n**Best practices by default:** Substratus models are immutable and contain\\ninformation about their lineage. Datasets are imported and snapshotted using\\noff-the-shelf manifests. Training executes in containerized environments, using\\nimmutable base artifacts. Inference servers are pre-configured to leverage\\nquantization on supported models. GitOps is built-in, not bolted-on.\\n\\n## **Guiding Principles**\\n\\nAs we continue to develop Substratus, we\u2019re grounded in the following guiding\\nprinciples:\\n\\n### **1. Prioritize Simplicity**\\n\\n We believe the importance of minimizing complexity in software cannot be\\n understated. In Substratus, we will work hard to keep complexity to a minimum\\n as the project grows. The Substratus API currently consists of 4 resource\\n types: Datasets, Models, Servers, and Notebooks. The project currently depends\\n on two cloud services outside of the cluster: a bucket and a container registry\\n (we are working on making these optional too). The project does not (and will\\n never) depend on a web of complex components like Istio.\\n\\n### **2. Prioritize UX**\\n\\nWe believe a company\u2019s most precious resource is their engineer\u2019s time.\\nSubstratus seeks to maximize the productivity of data scientists and engineers\\nthrough providing a best-in-class user experience. We strive to build a set of\\nwell-designed primitives that allow ML practitioners to enter a flow state as\\nthey move between importing data, training, and serving models.\\n\\n## **Roadmap**\\n\\nWe are fast at work adding new functionality, focused on creating the most\\nproductive and enjoyable platform for ML practitioners. Coming soon:\\n\\n1. Support for AWS and Azure\\n2. VS Code Notebook Integration\\n3. Large-scale distributed training\\n4. ML ecosystem integrations\\n\\nTry Substratus today in your GCP project by following the [quick start\\ndocs](https://www.substratus.ai/docs/quickstart). Let us know what features you\\nwould like to see on our [GitHub\\nrepo](https://github.com/substratusai/substratus) and don\u2019t forget to add a\\nstar!"}]}')}}]);