"use strict";(self.webpackChunksubstratus_website=self.webpackChunksubstratus_website||[]).push([[5081],{9281:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>g,frontMatter:()=>o,metadata:()=>r,toc:()=>d});var l=i(5893),t=i(1151),a=i(2719);const o={slug:"converting-hf-model-gguf-model",title:"Converting HuggingFace Models to GGUF/GGML",authors:[{name:"Sam Stoelinga",title:"Engineer",url:"https://github.com/samos123"}],tags:["llama.cpp","gguf"]},s=void 0,r={permalink:"/blog/converting-hf-model-gguf-model",editUrl:"https://github.com/substratusai/substratusai.github.io/tree/main/blog/2023-08-31-converting-hf-model-to-gguf-model.md",source:"@site/blog/2023-08-31-converting-hf-model-to-gguf-model.md",title:"Converting HuggingFace Models to GGUF/GGML",description:"Llama.cpp is a great way to run LLMs efficiently on CPUs and GPUs. The downside",date:"2023-08-31T00:00:00.000Z",formattedDate:"August 31, 2023",tags:[{label:"llama.cpp",permalink:"/blog/tags/llama-cpp"},{label:"gguf",permalink:"/blog/tags/gguf"}],readingTime:2.43,hasTruncateMarker:!1,authors:[{name:"Sam Stoelinga",title:"Engineer",url:"https://github.com/samos123"}],frontMatter:{slug:"converting-hf-model-gguf-model",title:"Converting HuggingFace Models to GGUF/GGML",authors:[{name:"Sam Stoelinga",title:"Engineer",url:"https://github.com/samos123"}],tags:["llama.cpp","gguf"]},unlisted:!1,prevItem:{title:"Tutorial: K8s Kind with GPUs",permalink:"/blog/kind-with-gpus"},nextItem:{title:"A Kind Local Llama on K8s",permalink:"/blog/kind-local-llama-on-rtx-2060"}},c={authorsImageUrls:[void 0]},d=[{value:"Downloading a HuggingFace model",id:"downloading-a-huggingface-model",level:3},{value:"Converting the model",id:"converting-the-model",level:3},{value:"Pushing the GGUF model to HuggingFace",id:"pushing-the-gguf-model-to-huggingface",level:3}];function h(e){const n={a:"a",code:"code",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,t.a)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.p,{children:"Llama.cpp is a great way to run LLMs efficiently on CPUs and GPUs. The downside\nhowever is that you need to convert models to a format that's supported by Llama.cpp,\nwhich is now the GGUF file format.  In this blog post you will learn how to convert\na HuggingFace model (Vicuna 13b v1.5) to GGUF model."}),"\n",(0,l.jsx)(n.p,{children:"At the time of writing, Llama.cpp supports\nthe following models:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"LLaMA \ud83e\udd99"}),"\n",(0,l.jsx)(n.li,{children:"LLaMA 2 \ud83e\udd99\ud83e\udd99"}),"\n",(0,l.jsx)(n.li,{children:"Falcon"}),"\n",(0,l.jsx)(n.li,{children:"Alpaca"}),"\n",(0,l.jsx)(n.li,{children:"GPT4All"}),"\n",(0,l.jsx)(n.li,{children:"Chinese LLaMA / Alpaca and Chinese LLaMA-2 / Alpaca-2"}),"\n",(0,l.jsx)(n.li,{children:"Vigogne (French)"}),"\n",(0,l.jsx)(n.li,{children:"Vicuna"}),"\n",(0,l.jsx)(n.li,{children:"Koala"}),"\n",(0,l.jsx)(n.li,{children:"OpenBuddy \ud83d\udc36 (Multilingual)"}),"\n",(0,l.jsx)(n.li,{children:"Pygmalion 7B / Metharme 7B"}),"\n",(0,l.jsx)(n.li,{children:"WizardLM"}),"\n",(0,l.jsx)(n.li,{children:"Baichuan-7B and its derivations (such as baichuan-7b-sft)"}),"\n",(0,l.jsx)(n.li,{children:"Aquila-7B / AquilaChat-7B"}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"At a high-level you will be going through the following steps:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Downloading a HuggingFace model"}),"\n",(0,l.jsxs)(n.li,{children:["Running llama.cpp ",(0,l.jsx)(n.code,{children:"convert.py"})," on the HuggingFace model"]}),"\n",(0,l.jsx)(n.li,{children:"(Optionally) Uploading the model back to HuggingFace"}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"downloading-a-huggingface-model",children:"Downloading a HuggingFace model"}),"\n",(0,l.jsxs)(n.p,{children:["There are various ways to download models, but in my experience the ",(0,l.jsx)(n.code,{children:"huggingface_hub"}),"\nlibrary has been the most reliable. The ",(0,l.jsx)(n.code,{children:"git clone"})," method occasionally results in\nOOM errors for large models."]}),"\n",(0,l.jsxs)(n.p,{children:["Install the ",(0,l.jsx)(n.code,{children:"huggingface_hub"})," library:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"pip install huggingface_hub\n"})}),"\n",(0,l.jsxs)(n.p,{children:["Create a Python script named ",(0,l.jsx)(n.code,{children:"download.py"})," with the following content:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from huggingface_hub import snapshot_download\nmodel_id="lmsys/vicuna-13b-v1.5"\nsnapshot_download(repo_id=model_id, local_dir="vicuna-hf",\n                  local_dir_use_symlinks=False, revision="main")\n'})}),"\n",(0,l.jsx)(n.p,{children:"Run the Python script:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"python download.py\n"})}),"\n",(0,l.jsxs)(n.p,{children:["You should now have the model downloaded to a directory called\n",(0,l.jsx)(n.code,{children:"vicuna-hf"}),". Verify by running:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"ls -lash vicuna-hf\n"})}),"\n",(0,l.jsx)(n.h3,{id:"converting-the-model",children:"Converting the model"}),"\n",(0,l.jsx)(n.p,{children:"Now it's time to convert the downloaded HuggingFace model to a GGUF model.\nLlama.cpp comes with a converter script to do this."}),"\n",(0,l.jsx)(n.p,{children:"Get the script by cloning the llama.cpp repo:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/ggerganov/llama.cpp.git\n"})}),"\n",(0,l.jsx)(n.p,{children:"Install the required python libraries:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"pip install -r llama.cpp/requirements.txt\n"})}),"\n",(0,l.jsx)(n.p,{children:"Verify the script is there and understand the various options:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"python llama.cpp/convert.py -h\n"})}),"\n",(0,l.jsx)(n.p,{children:"Convert the HF model to GGUF model:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"python llama.cpp/convert.py vicuna-hf \\\n  --outfile vicuna-13b-v1.5.gguf \\\n  --outtype q8_0\n"})}),"\n",(0,l.jsxs)(n.p,{children:["In this case we're also quantizing the model to 8 bit by setting\n",(0,l.jsx)(n.code,{children:"--outtype q8_0"}),". Quantizing helps improve inference speed, but it can\nnegatively impact quality.\nYou can use ",(0,l.jsx)(n.code,{children:"--outtype f16"})," (16 bit) or ",(0,l.jsx)(n.code,{children:"--outtype f32"})," (32 bit) to preserve original\nquality."]}),"\n",(0,l.jsx)(n.p,{children:"Verify the GGUF model was created:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"ls -lash vicuna-13b-v1.5.gguf\n"})}),"\n",(0,l.jsx)(n.h3,{id:"pushing-the-gguf-model-to-huggingface",children:"Pushing the GGUF model to HuggingFace"}),"\n",(0,l.jsx)(n.p,{children:"You can optionally push back the GGUF model to HuggingFace."}),"\n",(0,l.jsxs)(n.p,{children:["Create a Python script with the filename ",(0,l.jsx)(n.code,{children:"upload.py"})," that\nhas the following content:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from huggingface_hub import HfApi\napi = HfApi()\n\nmodel_id = "substratusai/vicuna-13b-v1.5-gguf"\napi.create_repo(model_id, exist_ok=True, repo_type="model")\napi.upload_file(\n    path_or_fileobj="vicuna-13b-v1.5.gguf",\n    path_in_repo="vicuna-13b-v1.5.gguf",\n    repo_id=model_id,\n)\n'})}),"\n",(0,l.jsxs)(n.p,{children:["Get a HuggingFace Token that has write permission from here:\n",(0,l.jsx)(n.a,{href:"https://huggingface.co/settings/tokens",children:"https://huggingface.co/settings/tokens"})]}),"\n",(0,l.jsx)(n.p,{children:"Set your HuggingFace token:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"export HUGGING_FACE_HUB_TOKEN=<paste-your-own-token>\n"})}),"\n",(0,l.jsxs)(n.p,{children:["Run the ",(0,l.jsx)(n.code,{children:"upload.py"})," script:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"python upload.py\n"})}),"\n",(0,l.jsxs)(n.p,{children:["Interested in learning how to automate flows like this? Checkout our\nopen source project:\n",(0,l.jsx)(a.Z,{href:"https://github.com/substratusai/substratus","data-icon":"octicon-star","data-size":"large","data-show-count":"true","aria-label":"Star substratusai/substratus on GitHub",children:"Star"})]})]})}function g(e={}){const{wrapper:n}={...(0,t.a)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(h,{...e})}):h(e)}},1151:(e,n,i)=>{i.d(n,{Z:()=>s,a:()=>o});var l=i(7294);const t={},a=l.createContext(t);function o(e){const n=l.useContext(a);return l.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),l.createElement(a.Provider,{value:n},e.children)}}}]);