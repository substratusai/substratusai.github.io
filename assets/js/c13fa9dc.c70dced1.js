"use strict";(self.webpackChunksubstratus_website=self.webpackChunksubstratus_website||[]).push([[9636],{3905:(t,e,a)=>{a.d(e,{Zo:()=>p,kt:()=>g});var n=a(7294);function l(t,e,a){return e in t?Object.defineProperty(t,e,{value:a,enumerable:!0,configurable:!0,writable:!0}):t[e]=a,t}function r(t,e){var a=Object.keys(t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(t);e&&(n=n.filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable}))),a.push.apply(a,n)}return a}function o(t){for(var e=1;e<arguments.length;e++){var a=null!=arguments[e]?arguments[e]:{};e%2?r(Object(a),!0).forEach((function(e){l(t,e,a[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(t,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(e){Object.defineProperty(t,e,Object.getOwnPropertyDescriptor(a,e))}))}return t}function s(t,e){if(null==t)return{};var a,n,l=function(t,e){if(null==t)return{};var a,n,l={},r=Object.keys(t);for(n=0;n<r.length;n++)a=r[n],e.indexOf(a)>=0||(l[a]=t[a]);return l}(t,e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(t);for(n=0;n<r.length;n++)a=r[n],e.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(t,a)&&(l[a]=t[a])}return l}var i=n.createContext({}),u=function(t){var e=n.useContext(i),a=e;return t&&(a="function"==typeof t?t(e):o(o({},e),t)),a},p=function(t){var e=u(t.components);return n.createElement(i.Provider,{value:e},t.children)},m="mdxType",c={inlineCode:"code",wrapper:function(t){var e=t.children;return n.createElement(n.Fragment,{},e)}},d=n.forwardRef((function(t,e){var a=t.components,l=t.mdxType,r=t.originalType,i=t.parentName,p=s(t,["components","mdxType","originalType","parentName"]),m=u(a),d=l,g=m["".concat(i,".").concat(d)]||m[d]||c[d]||r;return a?n.createElement(g,o(o({ref:e},p),{},{components:a})):n.createElement(g,o({ref:e},p))}));function g(t,e){var a=arguments,l=e&&e.mdxType;if("string"==typeof t||l){var r=a.length,o=new Array(r);o[0]=d;var s={};for(var i in e)hasOwnProperty.call(e,i)&&(s[i]=e[i]);s.originalType=t,s[m]="string"==typeof t?t:l,o[1]=s;for(var u=2;u<r;u++)o[u]=a[u];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}d.displayName="MDXCreateElement"},701:(t,e,a)=>{a.r(e),a.d(e,{assets:()=>u,contentTitle:()=>s,default:()=>d,frontMatter:()=>o,metadata:()=>i,toc:()=>p});var n=a(7462),l=(a(7294),a(3905)),r=a(2719);const o={slug:"kind-local-llama-on-rtx-2060",title:"A Kind Local Llama on K8s",authors:[{name:"Sam Stoelinga",title:"Engineer",url:"https://github.com/samos123"}],image:"/img/kind-llama-on-laptop-gpu.png",tags:["llama","kind"]},s=void 0,i={permalink:"/blog/kind-local-llama-on-rtx-2060",editUrl:"https://github.com/substratusai/substratusai.github.io/tree/main/blog/2023-08-25-a-kind-llama.md",source:"@site/blog/2023-08-25-a-kind-llama.md",title:"A Kind Local Llama on K8s",description:"A Llama 13B parameter model running on a laptop with a mere RTX 2060?!",date:"2023-08-25T00:00:00.000Z",formattedDate:"August 25, 2023",tags:[{label:"llama",permalink:"/blog/tags/llama"},{label:"kind",permalink:"/blog/tags/kind"}],readingTime:2.49,hasTruncateMarker:!1,authors:[{name:"Sam Stoelinga",title:"Engineer",url:"https://github.com/samos123"}],frontMatter:{slug:"kind-local-llama-on-rtx-2060",title:"A Kind Local Llama on K8s",authors:[{name:"Sam Stoelinga",title:"Engineer",url:"https://github.com/samos123"}],image:"/img/kind-llama-on-laptop-gpu.png",tags:["llama","kind"]},prevItem:{title:"Converting HuggingFace Models to GGUF/GGML",permalink:"/blog/converting-hf-model-gguf-model"},nextItem:{title:"Introducing: kubectl notebook",permalink:"/blog/introducing-kubectl-notebook"}},u={authorsImageUrls:[void 0]},p=[{value:"Create Kind K8s cluster with GPU support",id:"create-kind-k8s-cluster-with-gpu-support",level:3},{value:"Install Substratus",id:"install-substratus",level:3},{value:"Load the Llama 2 13b chat GGUF model",id:"load-the-llama-2-13b-chat-gguf-model",level:3},{value:"Serve the model",id:"serve-the-model",level:3}],m={toc:p},c="wrapper";function d(t){let{components:e,...a}=t;return(0,l.kt)(c,(0,n.Z)({},m,a,{components:e,mdxType:"MDXLayout"}),(0,l.kt)("img",{src:"/img/kind-llama-on-laptop-gpu.png",alt:"kubectl notebook",width:"100%"}),(0,l.kt)("p",null,"A Llama 13B parameter model running on a laptop with a mere RTX 2060?!\nYes, it all ran surprisingly well at around 7 tokens / sec.\nFollow along and learn how to do this on your environment."),(0,l.kt)("p",null,"My laptop setup looks like this:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Kind for deploying a single node K8s cluster"),(0,l.kt)("li",{parentName:"ul"},"AMD Ryzen 7 (8 threads), 16 GB system memory, RTX 2060 (6GB GPU memory)"),(0,l.kt)("li",{parentName:"ul"},"Llama.cpp/GGML for fast serving and loading larger models on consumer hardware")),(0,l.kt)("p",null,"You might be wondering: How can a model with 13 billion parameters fit into a 6GB GPU? You'd expect it to need about 13GB, especially if it's running in 4-bit mode, right?\nYes it should because 13 billion * 4 bytes / (32 bits / 4 bits) = 13 GB.\nBut thanks to ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/ggerganov/llama.cpp"},"Llama.cpp"),", we can load only parts of the model into the GPU. Plus, Llama.cpp can run efficiently just using the CPU."),(0,l.kt)("p",null,"Want to try this out yourself? Follow a long for a fun ride."),(0,l.kt)("h3",{id:"create-kind-k8s-cluster-with-gpu-support"},"Create Kind K8s cluster with GPU support"),(0,l.kt)("p",null,"Install the NVIDIA container toolkit for Docker: ",(0,l.kt)("a",{parentName:"p",href:"https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html"},"Install Guide")),(0,l.kt)("p",null,"Use the convenience script to create a Kind cluster and configure GPU support:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"bash <(curl https://raw.githubusercontent.com/substratusai/substratus/main/install/kind/up-gpu.sh)\n")),(0,l.kt)("p",null,"Or inspect the ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/substratusai/substratus/blob/main/install/kind/up-gpu.sh"},"script")," and run the steps one by one."),(0,l.kt)("h3",{id:"install-substratus"},"Install Substratus"),(0,l.kt)("p",null,"Install the Substratus K8s operator which will orchestrate model loading and serving:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl apply -f https://raw.githubusercontent.com/substratusai/substratus/main/install/kind/manifests.yaml\n")),(0,l.kt)("h3",{id:"load-the-llama-2-13b-chat-gguf-model"},"Load the Llama 2 13b chat GGUF model"),(0,l.kt)("p",null,"Create a Model resource to load the ",(0,l.kt)("a",{parentName:"p",href:"https://huggingface.co/substratusai/Llama-2-13B-chat-GGUF"},"Llama 2 13b chat GGUF model")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: substratus.ai/v1\nkind: Model\nmetadata:\n  name: llama2-13b-chat-gguf\nspec:\n  image: substratusai/model-loader-huggingface\n  params:\n    name: substratusai/Llama-2-13B-chat-GGUF\n    files: "model.bin"\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl apply -f https://raw.githubusercontent.com/substratusai/substratus/main/examples/llama2-13b-chat-gguf/base-model.yaml\n")),(0,l.kt)("p",null,"The model is being downloaded from HuggingFace into your Kind cluster."),(0,l.kt)("h3",{id:"serve-the-model"},"Serve the model"),(0,l.kt)("p",null,"Create a Server resource to serve the model:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: substratus.ai/v1\nkind: Server\nmetadata:\n  name: llama2-13b-chat-gguf\nspec:\n  image: substratusai/model-server-llama-cpp:latest-gpu\n  model:\n    name: llama2-13b-chat-gguf\n  params:\n    n_gpu_layers: 30\n  resources:\n    gpu:\n      count: 1\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl apply -f https://raw.githubusercontent.com/substratusai/substratus/main/examples/llama2-13b-chat-gguf/server-gpu.yaml\n")),(0,l.kt)("p",null,"Note in my case 30 out of 42 layers loaded into GPU is the max, but you might be able\nto load all 42 layers into the GPU if you have more GPU memory."),(0,l.kt)("p",null,"Once the model is ready it will start serving an OpenAI compatible\nAPI endpoint."),(0,l.kt)("p",null,"Expose the Server to a local port by using port forwarding:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl port-forward service/llama2-13b-chat-gguf-server 8080:8080\n")),(0,l.kt)("p",null,"Let's throw some prompts at it:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'curl http://localhost:8080/v1/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{ "prompt": "Who was the first president of the United States?", "stop": ["."]}\'\n')),(0,l.kt)("p",null,"Checkout the full API docs here: ",(0,l.kt)("a",{parentName:"p",href:"http://localhost:8080/docs"},"http://localhost:8080/docs")),(0,l.kt)("p",null,"You can play around with other models. For example, if you have a 24 GB GPU card you should\nbe able to run Llama 2 70B in 4 bit mode by using llama.cpp."),(0,l.kt)("p",null,"Support the project by adding a star on GitHub! \u2764\ufe0f"),(0,l.kt)(r.Z,{href:"https://github.com/substratusai/substratus","data-icon":"octicon-star","data-size":"large","data-show-count":"true","aria-label":"Star substratusai/substratus on GitHub",mdxType:"GitHubButton"},"Star"))}d.isMDXComponent=!0}}]);