"use strict";(self.webpackChunksubstratus_website=self.webpackChunksubstratus_website||[]).push([[7629],{2891:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>o,toc:()=>c});var s=n(5893),i=n(1151);const a={slug:"mistral-7b-instruct-k8s-helm-tgi",title:"Deploying Mistral 7B Instruct on K8s using TGI",authors:[{name:"Sam Stoelinga",title:"Engineer",url:"https://github.com/samos123"}],tags:["k8s","mistral-7b","helm"],image:"/img/mistral-7b-helm-k8s-tgi.png"},r=void 0,o={permalink:"/blog/mistral-7b-instruct-k8s-helm-tgi",editUrl:"https://github.com/substratusai/substratusai.github.io/tree/main/blog/2023-10-21-mistral-7b-instruct-k8s-tgi.md",source:"@site/blog/2023-10-21-mistral-7b-instruct-k8s-tgi.md",title:"Deploying Mistral 7B Instruct on K8s using TGI",description:"Learn how to use the text-generation-inference (TGI) Helm Chart to quickly",date:"2023-10-21T00:00:00.000Z",formattedDate:"October 21, 2023",tags:[{label:"k8s",permalink:"/blog/tags/k-8-s"},{label:"mistral-7b",permalink:"/blog/tags/mistral-7-b"},{label:"helm",permalink:"/blog/tags/helm"}],readingTime:2.91,hasTruncateMarker:!1,authors:[{name:"Sam Stoelinga",title:"Engineer",url:"https://github.com/samos123"}],frontMatter:{slug:"mistral-7b-instruct-k8s-helm-tgi",title:"Deploying Mistral 7B Instruct on K8s using TGI",authors:[{name:"Sam Stoelinga",title:"Engineer",url:"https://github.com/samos123"}],tags:["k8s","mistral-7b","helm"],image:"/img/mistral-7b-helm-k8s-tgi.png"},unlisted:!1,prevItem:{title:"Calculating GPU memory for serving LLMs",permalink:"/blog/calculating-gpu-memory-for-llm"},nextItem:{title:"The K8s YAML dataset",permalink:"/blog/k8s-yaml-dataset"}},l={authorsImageUrls:[void 0]},c=[];function d(e){const t={a:"a",br:"br",code:"code",p:"p",pre:"pre",...(0,i.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)("img",{src:"/img/mistral-7b-helm-k8s-tgi.png",alt:"mistral 7b k8s helm",width:"100%"}),"\n",(0,s.jsx)(t.p,{children:"Learn how to use the text-generation-inference (TGI) Helm Chart to quickly\ndeploy Mistral 7B Instruct on your K8s cluster."}),"\n",(0,s.jsx)(t.p,{children:"Add the Substratus.ai Helm repo:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-bash",children:"helm repo add substratusai https://substratusai.github.io/helm\n"})}),"\n",(0,s.jsxs)(t.p,{children:["This command adds a new Helm repository, making the ",(0,s.jsx)(t.code,{children:"text-generation-inference"}),"\nHelm chart available for installation."]}),"\n",(0,s.jsxs)(t.p,{children:["Create a configuration file named ",(0,s.jsx)(t.code,{children:"values.yaml"}),". This file will contain the necessary settings for your deployment. Here\u2019s an example of what the content should look like:"]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{children:"model: mistralai/Mistral-7B-Instruct-v0.1\n# resources: # optional, override if you need more than 1 GPU\n#   limits:\n#     nvidia.com/gpu: 1\n# nodeSelector: # optional, can be used to target specific GPUs\n#   cloud.google.com/gke-accelerator: nvidia-l4\n"})}),"\n",(0,s.jsx)(t.p,{children:"In this configuration file, you are specifying the model to be deployed and optionally setting resource limits or targeting specific nodes based on your requirements."}),"\n",(0,s.jsx)(t.p,{children:"With your configuration file ready, you can now deploy Mistral 7B Instruct using Helm:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{children:"helm install mistral-7b-instruct substratusai/text-generation-inference \\\n    -f values.yaml\n"})}),"\n",(0,s.jsxs)(t.p,{children:["This command initiates the deployment, creating a Kubernetes Deployment and Service based on the settings defined in your ",(0,s.jsx)(t.code,{children:"values.yaml"})," file."]}),"\n",(0,s.jsx)(t.p,{children:"After initiating the deployment, it's important to ensure that everything is running as expected. Run the following command to get detailed information about the newly created pod:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-bash",children:"kubectl describe pod -l app.kubernetes.io/instance=mistral-7b-instruct\n"})}),"\n",(0,s.jsx)(t.p,{children:"This will display various details about the pod, helping you to confirm that it has been successfully created and is in the right state. Note that depending on your cluster's setup, you might need to wait for the cluster autoscaler to provision additional resources if necessary."}),"\n",(0,s.jsx)(t.p,{children:"Once the pod is running, check the logs to ensure that the model is initializing properly:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-bash",children:"kubectl logs -f -l app.kubernetes.io/instance=mistral-7b-instruct\n"})}),"\n",(0,s.jsx)(t.p,{children:"The model first downloads the model and after a few minutes, you should\nsee a message that looks like this:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-text",children:"Invalid hostname, defaulting to 0.0.0.0\n"})}),"\n",(0,s.jsx)(t.p,{children:"This is expected and means it's now serving on host 0.0.0.0."}),"\n",(0,s.jsx)(t.p,{children:"By default, the model is only accessible within the Kubernetes cluster. To access it from your local machine, set up a port forward:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-bash",children:"kubectl port-forward deployments/mistral-7b-instruct-text-generation-inference 8080:8080\n"})}),"\n",(0,s.jsx)(t.p,{children:"This command maps port 8080 on your local machine to port 8080 on the deployed pod, allowing you to interact with the model directly."}),"\n",(0,s.jsxs)(t.p,{children:["With the service exposed, you can now run inference tasks. To explore the available API endpoints and their usage, visit the TGI API documentation at ",(0,s.jsx)(t.a,{href:"http://localhost:8080/docs",children:"http://localhost:8080/docs"}),"."]}),"\n",(0,s.jsx)(t.p,{children:"Here\u2019s an example of how to use curl to run an inference task:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-bash",children:'curl 127.0.0.1:8080/generate -X POST \\\n    -H \'Content-Type: application/json\' \\\n    --data-binary @- << \'EOF\' | jq -r \'.generated_text\'\n{\n    "inputs": "<s>[INST] Write a K8s YAML file to create a pod that deploys nginx[/INST]",\n    "parameters": {"max_new_tokens": 400}\n}\nEOF\n'})}),"\n",(0,s.jsx)(t.p,{children:"In this example, we are instructing the model to generate a Kubernetes YAML file for deploying an Nginx pod. The prompt includes specific tokens that the Mistral 7B Instruct model recognizes, ensuring accurate and context-aware responses."}),"\n",(0,s.jsxs)(t.p,{children:["The prompt we are using starts with ",(0,s.jsx)(t.code,{children:"<s>"})," token which indicates beginning of a sequence.\nThe ",(0,s.jsx)(t.code,{children:"[INST]"})," token tells Mistral-7b Instruct what follows is an instruction. The Mistral 7B\nInstruct model was finetuned with this prompt template, so it's important to re-use that\nsame prompt template."]}),"\n",(0,s.jsx)(t.p,{children:"The response is quite impressive, it did return a valid K8s YAML manifest\nand also instructions on how to apply it."}),"\n",(0,s.jsxs)(t.p,{children:["Need help? Want to see other models? other serving frameworks?",(0,s.jsx)(t.br,{}),"\n","Join our Discord and ask me directly:"]}),"\n",(0,s.jsx)("a",{href:"https://discord.gg/JeXhcmjZVm",children:(0,s.jsx)("img",{alt:"discord-invite",src:"https://dcbadge.vercel.app/api/server/JeXhcmjZVm?style=flat"})})]})}function h(e={}){const{wrapper:t}={...(0,i.a)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},1151:(e,t,n)=>{n.d(t,{Z:()=>o,a:()=>r});var s=n(7294);const i={},a=s.createContext(i);function r(e){const t=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(a.Provider,{value:t},e.children)}}}]);