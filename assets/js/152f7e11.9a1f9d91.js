"use strict";(self.webpackChunksubstratus_website=self.webpackChunksubstratus_website||[]).push([[536],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>b});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function s(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?s(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):s(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},s=Object.keys(e);for(a=0;a<s.length;a++)n=s[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(a=0;a<s.length;a++)n=s[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var i=a.createContext({}),u=function(e){var t=a.useContext(i),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},c=function(e){var t=u(e.components);return a.createElement(i.Provider,{value:t},e.children)},d="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,s=e.originalType,i=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),d=u(n),m=r,b=d["".concat(i,".").concat(m)]||d[m]||p[m]||s;return n?a.createElement(b,o(o({ref:t},c),{},{components:n})):a.createElement(b,o({ref:t},c))}));function b(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var s=n.length,o=new Array(s);o[0]=m;var l={};for(var i in t)hasOwnProperty.call(t,i)&&(l[i]=t[i]);l.originalType=e,l[d]="string"==typeof e?e:r,o[1]=l;for(var u=2;u<s;u++)o[u]=n[u];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},3180:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>i,contentTitle:()=>o,default:()=>p,frontMatter:()=>s,metadata:()=>l,toc:()=>u});var a=n(7462),r=(n(7294),n(3905));const s={sidebar_position:2},o="Deploying and Finetuning Falcon-7b-instruct",l={unversionedId:"tutorials/deploying-finetuning-falcon-7b-instruct",id:"tutorials/deploying-finetuning-falcon-7b-instruct",title:"Deploying and Finetuning Falcon-7b-instruct",description:"In this tutorial, you will learn how to deploy and finetune the Falcon-7b-instruct model",source:"@site/docs/tutorials/deploying-finetuning-falcon-7b-instruct.md",sourceDirName:"tutorials",slug:"/tutorials/deploying-finetuning-falcon-7b-instruct",permalink:"/docs/tutorials/deploying-finetuning-falcon-7b-instruct",draft:!1,editUrl:"https://github.com/substratusai/substratusai.github.io/tree/main/docs/tutorials/deploying-finetuning-falcon-7b-instruct.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Tutorials",permalink:"/docs/category/tutorials"},next:{title:"Architecture",permalink:"/docs/architecture"}},i={},u=[{value:"Loading the falcon-7b-instruct model into Substratus",id:"loading-the-falcon-7b-instruct-model-into-substratus",level:2},{value:"Serving the loaded model",id:"serving-the-loaded-model",level:2},{value:"Finetuning falcon-7b-instruct",id:"finetuning-falcon-7b-instruct",level:2},{value:"Loading a custom dataset",id:"loading-a-custom-dataset",level:3},{value:"Creating the fine tuned model",id:"creating-the-fine-tuned-model",level:3}],c={toc:u},d="wrapper";function p(e){let{components:t,...n}=e;return(0,r.kt)(d,(0,a.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"deploying-and-finetuning-falcon-7b-instruct"},"Deploying and Finetuning Falcon-7b-instruct"),(0,r.kt)("p",null,"In this tutorial, you will learn how to deploy and finetune the Falcon-7b-instruct model\nwith a custom dataset. The tutorial will cover 3 main sections:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Loading the Falcon-7b-instruct base model"),(0,r.kt)("li",{parentName:"ol"},"Creating a server to serve the model"),(0,r.kt)("li",{parentName:"ol"},"Loading a dataset and finetuning the base model")),(0,r.kt)("p",null,"Prereq:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Install Substratus following the ",(0,r.kt)("a",{parentName:"li",href:"/docs/installation"},"installation guide"))),(0,r.kt)("h2",{id:"loading-the-falcon-7b-instruct-model-into-substratus"},"Loading the falcon-7b-instruct model into Substratus"),(0,r.kt)("p",null,"Create a Model resource to load the model from HuggingFace into Substratus."),(0,r.kt)("p",null,"Create the model by running:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"}," kubectl apply -f https://raw.githubusercontent.com/substratusai/substratus/main/examples/falcon-7b-instruct/base-model.yaml\n")),(0,r.kt)("p",null,"Once you create the model, it will create a K8s job to load the HuggingFace model\ninto Substratus. This job will use the container image defined in the Model resource to\nload the HuggingFace model into Cloud Storage. For example, if you're using GCP,\nthen it would load the model into a GCS bucket."),(0,r.kt)("p",null,"Let's take a look at the Model resource that you just applied:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"}," curl https://raw.githubusercontent.com/substratusai/substratus/main/examples/falcon-7b-instruct/base-model.yaml\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"apiVersion: substratus.ai/v1\nkind: Model\nmetadata:\n  name: falcon-7b-instruct\nspec:\n  image:\n    name: substratusai/model-loader-huggingface\n  params:\n    name: tiiuae/falcon-7b-instruct\n")),(0,r.kt)("p",null,"You can look at the logs of the loader job by running:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"}," kubectl logs jobs/falcon-7b-instruct-modeller | tail -n 10\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"[NbConvertApp] content: {'execution_state': 'busy'}\n[NbConvertApp] msg_type: execute_input\n[NbConvertApp] content: {'code': '! ls -lash /model/saved', 'execution_count': 5}\n[NbConvertApp] msg_type: stream\n[NbConvertApp] content: {'name': 'stdout', 'text': 'total 14G\\r\\n1.5K -rw-r--r-- 1 root 3003 1.5K Jul 15 02:46 .gitattributes\\r\\n 10K -rw-r--r-- 1 root 3003 9.6K Jul 15 02:46 README.md\\r\\n1.0K -rw-r--r-- 1 root 3003  667 Jul 15 02:46 config.json\\r\\n3.0K -rw-r--r-- 1 root 3003 2.6K Jul 15 02:46 configuration_RW.py\\r\\n 512 -rw-r--r-- 1 root 3003  111 Jul 15 02:46 generation_config.json\\r\\n1.5K -rw-r--r-- 1 root 3003 1.2K Jul 15 02:46 handler.py\\r\\n 47K -rw-r--r-- 1 root 3003  47K Jul 15 02:46 modelling_RW.py\\r\\n9.3G -rw-r--r-- 1 root 3003 9.3G Jul 15 02:48 pytorch_model-00001-of-00002.bin\\r\\n4.2G -rw-r--r-- 1 root 3003 4.2G Jul 15 02:47 pytorch_model-00002-of-00002.bin\\r\\n 17K -rw-r--r-- 1 root 3003  17K Jul 15 02:46 pytorch_model.bin.index.json\\r\\n 512 -rw-r--r-- 1 root 3003  281 Jul 15 02:46 special_tokens_map.json\\r\\n2.7M -rw-r--r-- 1 root 3003 2.7M Jul 15 02:46 tokenizer.json\\r\\n 512 -rw-r--r-- 1 root 3003  220 Jul 15 02:46 tokenizer_config.json\\r\\n'}\n[NbConvertApp] msg_type: status\n[NbConvertApp] content: {'execution_state': 'idle'}\n[NbConvertApp] Destroying zmq context for <jupyter_client.asynchronous.client.AsyncKernelClient object at 0x7f101990c650>\n[NbConvertApp] Applying preprocessor: coalesce_streams\n[NbConvertApp] Writing 9098 bytes to /model/logs/load.ipynb\n")),(0,r.kt)("p",null,"After about 5 minutes the job should finish and the Model resource should report the status\nto be ready. Verify by running:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"}," kubectl describe model falcon-7b-instruct\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Name:         falcon-7b-instruct\nNamespace:    default\nLabels:       <none>\nAnnotations:  <none>\nAPI Version:  substratus.ai/v1\nKind:         Model\nMetadata:\n  Creation Timestamp:  2023-07-15T02:46:05Z\n  Generation:          1\n  Resource Version:    14266797\n  UID:                 077198a0-32ec-4f07-9bc3-ba3a1f1a3729\nSpec:\n  Image:\n    Name:  substratusai/model-loader-huggingface\n  Params:\n    Name:  tiiuae/falcon-7b-instruct\nStatus:\n  Conditions:\n    Last Transition Time:  2023-07-15T02:51:18Z\n    Message:               \n    Observed Generation:   1\n    Reason:                JobComplete\n    Status:                True\n    Type:                  Modelled\n  Ready:                   true\n  URL:                     gs://substratus-models/077198a0-32ec-4f07-9bc3-ba3a1f1a3729/\nEvents:                    <none>\n")),(0,r.kt)("h2",{id:"serving-the-loaded-model"},"Serving the loaded model"),(0,r.kt)("p",null,"The Substratus Server resource lets you serve models that were loaded into Substratus.\nSubstratus provides a serving image that uses Basaran to provide an OpenAI\ncompatible API endpoint and also a Web UI which is compatible with most of the\nLarge Language Models on HuggingFace."),(0,r.kt)("p",null,"Create the Server resource by running:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"}," kubectl apply -f https://raw.githubusercontent.com/substratusai/substratus/main/examples/falcon-7b-instruct/server.yaml\n")),(0,r.kt)("p",null,"The following Server resource is used:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: substratus.ai/v1\nkind: Server\nmetadata:\n  name: falcon-7b-instruct\nspec:\n  image:\n    name: substratusai/model-server-basaran\n  model:\n    name: falcon-7b-instruct\n  resources:\n    gpu:\n      type: nvidia-l4\n      count: 1\n")),(0,r.kt)("p",null,"In the Model resource spec the following things are configured:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"image.name: This is the image published by Substratus that can serve models."),(0,r.kt)("li",{parentName:"ol"},"model.name: Refers to the name of the model that was loaded earlier in this tutorial"),(0,r.kt)("li",{parentName:"ol"},"resources: These specify what kind of resources are needed to serve the model. The Falcon-7b model requires GPUs to perform decently. In this case, 1 NVidia L4 GPU is requested.")),(0,r.kt)("p",null,"It takes about 5 minutes to pull the container, load the model into GPU memory and being ready to serve requests. You can check if the Server is ready by running:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"}," kubectl describe server falcon-7b-instruct\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Name:         falcon-7b-instruct\nNamespace:    default\nLabels:       <none>\nAnnotations:  <none>\nAPI Version:  substratus.ai/v1\nKind:         Server\nMetadata:\n  Creation Timestamp:  2023-07-15T05:43:56Z\n  Generation:          1\n  Resource Version:    14364432\n  UID:                 f130adc7-9243-4a19-b195-ef8b14c8b3ac\nSpec:\n  Image:\n    Name:  substratusai/model-server-basaran\n  Model:\n    Name:  falcon-7b-instruct\n  Resources:\n    Cpu:   2\n    Disk:  10\n    Gpu:\n      Count:  1\n      Type:   nvidia-l4\n    Memory:   10\nStatus:\n  Conditions:\n    Last Transition Time:  2023-07-15T05:48:32Z\n    Message:               \n    Observed Generation:   1\n    Reason:                DeploymentReady\n    Status:                True\n    Type:                  Deployed\n  Ready:                   true\nEvents:                    <none>\n")),(0,r.kt)("p",null,"By default Substratus creates a K8s Service to expose the Server, however this Service is of type ClusterIP, which means you can not directly access it over the internet. So let's use K8s Port Forwarding to access the server."),(0,r.kt)("p",null,"Run the following command to forward your local 8080 port to the Server port 8080:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"}," kubectl port-forward service/falcon-7b-instruct-server 8080:8080\n")),(0,r.kt)("p",null,"You should now be able to access the web interface of the Server by going to\n",(0,r.kt)("a",{parentName:"p",href:"http://localhost:8080"},"http://localhost:8080")),(0,r.kt)("p",null,"You have now deployed the falcon-7b-instruct model."),(0,r.kt)("h2",{id:"finetuning-falcon-7b-instruct"},"Finetuning falcon-7b-instruct"),(0,r.kt)("p",null,"The base model is pretty decent but it won't be so helpful on domain\nspecific instructions that it hasn't been trained on.\nFinetuning with your own dataset can help to make the model work well\nfor specific use cases."),(0,r.kt)("p",null,"In Substratus the Model resource and Dataset resource can be used\nto fine tune a base model."),(0,r.kt)("h3",{id:"loading-a-custom-dataset"},"Loading a custom dataset"),(0,r.kt)("p",null,"For this tutorial, a K8s instruction dataset will be used. The dataset\ncontains prompts and completions. Example entry in the dataset:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'prompt: Write YAML that defines a Pod named \\"dnsutils\\" in the \\"default\\" namespace\ncompletion: <K8s valid YAML file that defines K8s resources>\n')),(0,r.kt)("p",null,"The goal here is to train a model that is able to generate valid K8s YAML files based on the prompt given."),(0,r.kt)("p",null,"The dataset loader used is available here: ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/substratusai/dataset-k8s-instructions"},"https://github.com/substratusai/dataset-k8s-instructions")),(0,r.kt)("p",null,"Create the Dataset resource to load the dataset into Substratus:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"}," kubectl apply -f https://raw.githubusercontent.com/substratusai/substratus/main/examples/datasets/k8s-instructions.yaml\n")),(0,r.kt)("p",null,"It takes a few minutes to load the dataset. Verify the Dataset is ready:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"}," kubectl describe dataset k8s-instructions\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Name:         k8s-instructions\nNamespace:    default\nLabels:       <none>\nAnnotations:  <none>\nAPI Version:  substratus.ai/v1\nKind:         Dataset\nMetadata:\n  Creation Timestamp:  2023-07-12T05:38:49Z\n  Generation:          2\n  Resource Version:    12005235\n  UID:                 ced22d70-e7a3-4e1c-9d9e-09278c70681e\nSpec:\n  Command:\n    load.sh\n  Filename:  k8s-instructions.jsonl\n  Image:\n    Git:\n      URL:  https://github.com/substratusai/dataset-k8s-instructions\n    Name:   us-central1-docker.pkg.dev/my-gcp-project/substratus/dataset-default-k8s-instructions\nStatus:\n  Conditions:\n    Last Transition Time:  2023-07-12T05:39:18Z\n    Message:               Builder Job completed: k8s-instructions-dataset-container-builder\n    Observed Generation:   2\n    Reason:                JobComplete\n    Status:                True\n    Type:                  Built\n    Last Transition Time:  2023-07-12T05:40:05Z\n    Message:               \n    Observed Generation:   2\n    Reason:                JobComplete\n    Status:                True\n    Type:                  Loaded\n  Ready:                   true\n  URL:                     gs://my-gcp-project-substratus-datasets/ced22d70-e7a3-4e1c-9d9e-09278c70681e/data/k8s-instructions.jsonl\nEvents:                    <none>\n")),(0,r.kt)("h3",{id:"creating-the-fine-tuned-model"},"Creating the fine tuned model"),(0,r.kt)("p",null,"Fine tuned model = base model + dataset. Substratus makes this very easy by creating\nnew Model resource where specify the base model and the dataset, resulting in a\nnew fine tuned model."),(0,r.kt)("p",null,"Create the fined model by running:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"}," kubectl apply -f https://raw.githubusercontent.com/substratusai/substratus/main/examples/falcon-7b-instruct/finetuned-model.yaml\n")),(0,r.kt)("p",null,"The following Model resource was used in the above command:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: substratus.ai/v1\nkind: Model\nmetadata:\n  name: falcon-7b-instruct-k8s\nspec:\n  image:\n    name: substratusai/model-trainer-huggingface\n  baseModel:\n    name: falcon-7b-instruct\n  trainingDataset:\n    name: k8s-instructions\n  params:\n    epochs: 1\n  resources:\n    gpu:\n      count: 4\n      type: nvidia-l4\n")),(0,r.kt)("p",null,"Here we can see baseModel referring to the previously loaded model and trainingDataset refers\nto the dataset that was loaded in prior step. For training you generally need more GPUs, so that's\nwhy 4 GPUs are being requested."),(0,r.kt)("p",null,"Another key difference from the Model resource that was used to load the base model is the image that's being used. In this Model the image used is a HuggingFace based trainer image. The image\nis built and published by Substratus and the source is available here:\n",(0,r.kt)("a",{parentName:"p",href:"https://github.com/substratusai/model-trainer-huggingface"},"https://github.com/substratusai/model-trainer-huggingface")),(0,r.kt)("p",null,"The training takes about 20 to 30 minutes. You can watch the progress by running:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"}," kubectl logs jobs/falcon-7b-instruct-k8s-modeller | tail -n 5\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"[NbConvertApp] content: {'data': {'text/plain': 'Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]', 'application/vnd.jupyter.widget-view+json': {'version_major': 2, 'version_minor': 0, 'model_id': 'd32cd19e121147368d18bd6dee667365'}}, 'metadata': {}, 'transient': {}}\n[NbConvertApp] msg_type: comm_msg\n[NbConvertApp] content: {'data': {'method': 'update', 'state': {'value': 'Loading checkpoint shards:   0%'}, 'buffer_paths': []}, 'comm_id': '16a9b5f3e1584d6e952a78ab42350951'}\n[NbConvertApp] msg_type: comm_msg\n[NbConvertApp] content: {'data': {'method': 'update', 'state': {'value': ' 0/2 [00:00&lt;?, ?it/s]'}, 'buffer_paths': []}, 'comm_id': 'a2513eda3ca14e97844704daa51a3c97'}\n")),(0,r.kt)("p",null,"Wait until the falcon-7b-instruct-k8s-modeller job has finished. Once finished you can create another Server but this time let's specify our fine tuned model in the server object."),(0,r.kt)("p",null,"Create a Server to serve the falcon-7b-instruct-k8s finetuned model:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"}," kubectl apply -f https://raw.githubusercontent.com/substratusai/substratus/main/examples/falcon-7b-instruct/finetuned-server.yaml\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"server.substratus.ai/falcon-7b-instruct-k8s created\n")),(0,r.kt)("p",null,"Verify that the Server is ready by running:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"}," kubectl describe server falcon-7b-instruct-k8s\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Name:         falcon-7b-instruct-k8s\nNamespace:    default\nLabels:       <none>\nAnnotations:  <none>\nAPI Version:  substratus.ai/v1\nKind:         Server\nMetadata:\n  Creation Timestamp:  2023-07-15T21:13:17Z\n  Generation:          1\n  Resource Version:    14869700\n  UID:                 53c5e9a6-7535-4051-b3dc-d205f1efe5f0\nSpec:\n  Image:\n    Name:  substratusai/model-server-basaran\n  Model:\n    Name:  falcon-7b-instruct-k8s\n  Resources:\n    Cpu:   2\n    Disk:  10\n    Gpu:\n      Count:  1\n      Type:   nvidia-l4\n    Memory:   10\nStatus:\n  Conditions:\n    Last Transition Time:  2023-07-15T21:17:47Z\n    Message:               \n    Observed Generation:   1\n    Reason:                DeploymentReady\n    Status:                True\n    Type:                  Deployed\n  Ready:                   true\nEvents:                    <none>\n")),(0,r.kt)("p",null,"Port forward your localhost port 8081 to the Server that's serving the fine tuned model. The Server always uses port 8080 and you can choose which local port you want to use."),(0,r.kt)("p",null,"Run the following command (8081 is local and 8080 is remote):"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"}," kubectl port-forward services/falcon-7b-instruct-k8s-server 8081:8080\n")),(0,r.kt)("p",null,"You should now be able to access the web interface of the Server by going to\n",(0,r.kt)("a",{parentName:"p",href:"http://localhost:8081"},"http://localhost:8081")),(0,r.kt)("p",null,"You have now deployed the finetuned falcon-7b-instruct-k8s model that is better at generating K8s YAML manifests."),(0,r.kt)("p",null,"Next steps:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Read more about ",(0,r.kt)("a",{parentName:"li",href:"/docs/walkthrough/loading-datasets"},"loading datasets")),(0,r.kt)("li",{parentName:"ul"},"Read more about ",(0,r.kt)("a",{parentName:"li",href:"/docs/walkthrough/finetuning-models"},"finetuning models")),(0,r.kt)("li",{parentName:"ul"},"Read more about ",(0,r.kt)("a",{parentName:"li",href:"/docs/walkthrough/serving-models"},"serving models"))))}p.isMDXComponent=!0}}]);